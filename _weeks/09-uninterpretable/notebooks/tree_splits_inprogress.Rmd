---
title: "Tree splitting rules"
author: "Joshua Loftus"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)  
library(modelr)     
library(gapminder)  
theme_set(theme_minimal(base_size = 22))
```

## Sorting preliminaries

Check the documentation for `?sort` and `?order` and then write code to output the values of `Y` sorted according to the order of `X`

```{r}
X <- runif(10)
Y <- X + rnorm(10)
qplot(X, Y)
```

Base R solution

```{r}
Y[order(X)]
```


#### Check with tidyverse solution

```{r}
egdf <- data.frame(X = X, Y = Y)
egdf %>%
  arrange(X)
```

## Within-leaf averages

Below is some code that computes the average values of `Y` above and below a given split point

Base R

```{r}
x_split <- 0.8
c(mean(Y[X <= x_split]),
  mean(Y[X > x_split]))
```

tidyverse

```{r}
egdf %>%
  group_by(X <= x_split) %>%
  summarise(avg_Y = mean(Y))
```

#### How much computation is required if we change the value of `x_split`?

Search through all Y values every time -- sorting k items takes k*log(k) computation

#### Write code that sorts the data on `X` only once, and then, taking each `X` value as a split point consecutively, computes the average `Y` values above and below that split point while minimizing unnecessary computation

```{r}
X_order <- order(X)
Y_sorted <- Y[X_order]
n <- length(Y)
for (i in 2:(n-2)) {
  Yhat_below <- mean(Y_sorted[1:i])
  Yhat_above <- mean(Y_sorted[(i+1):n])
  RSS_below <- sum((Y_sorted[1:i] - Yhat_below)^2)
  RSS_above <- sum((Y_sorted[(i+1):n] - Yhat_above)^2)
  print(RSS_below + RSS_above)
}
```

#### How can we use this to decide the split point for a regression tree?

Compute the RSS of the predictor and choose the cutpoint with lowest RSS

#### How could we change this code to guarantee a minimum number of observations in each leaf?

## Numeric predictor

Write a function that inputs a single numeric predictor and outcome, and outputs a splitting point that achieves the lowest RSS

```{r}
# Output the x value for the split point
tree_split <- function(X, Y, min.obs) {
  X_order <- order(X)
  X_sorted <- X[X_order]
  Y_sorted <- Y[X_order]
  n <- length(Y)
  min_RSS <- Inf
  min_i <- 0
  for (i in min.obs:(n-min.obs)) {
    Yhat_below <- mean(Y_sorted[1:i])
    Yhat_above <- mean(Y_sorted[(i+1):n])
    RSS_below <- sum((Y_sorted[1:i] - Yhat_below)^2)
    RSS_above <- sum((Y_sorted[(i+1):n] - Yhat_above)^2)
    RSS_total <- RSS_below + RSS_above
    if (RSS_total < min_RSS) {
      min_RSS <- RSS_total
      min_i <- i
#      print("new minimum!")
    }
  }
  X_sorted[min_i]
}
tree_split(X, Y, 2)
```

### Example data

```{r}
n <- 1000
mixture_ids <- rbinom(n, 1, .5)
x <- rnorm(n) + 10*mixture_ids
y <- rnorm(n) + 10*mixture_ids
qplot(x,y)
```

#### Apply your function to the example data. Try different values of minimum observations per leaf

```{r}
tree_split(x, y, min.obs = 30)
```

#### Change the data generating process to make the example easier/harder and repeat the above initial split

```{r}

```

```{r}

```

#### Compare the initial tree split to a Bayes decision boundary in one of the above examples. Increase `n` and repeat

#### Test the function on some `gapminder` data, plot the initial split point

```{r}

```

```{r}

```


## Multiple splits

```{r}
n <- 1000
mixture_ids <- rbinom(n, 1, .5)
x <- rnorm(n) + 3*mixture_ids
y <- rnorm(n) + 3*mixture_ids
x <- c(x, rnorm(n/2, mean = -2))
y <- c(y, rnorm(n/2, mean = 5))
egdf <- data.frame(x = x, y = y)
egplot <- egdf %>%
  ggplot(aes(x, y)) +
  geom_point()
egplot
```

#### Use your single split function once, then split the data and apply the function again on each subset

```{r}

```

```{r}

```

```{r}

```

```{r}

```

#### Plot the data and each split point

```{r}

```


# Extra practice

## Recursive splits

#### Write a function taking data and a maximum number of splits as inputs, and outputting a decision tree

#### How does computation scale in number of levels (tree depth)?

## Categorical predictors

#### Write a function for when the outcome variable is categorical and some splitting rule based on e.g. Gini index or entropy



