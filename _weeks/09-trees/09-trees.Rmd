---
title: "9 Less interpretable methods"
description: |
  Neural networks and ensemble methods like bagging, random forests, and boosting can greatly increase predictive accuracy at the cost of ease of interpretation.
author:
  - name: Joshua Loftus
date: 10-03-2021
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE)
library(tidyverse)
theme_set(theme_minimal(base_size = 14))
```


## Materials

| Link | Type | Description             |
|:----:|:----:|:------------------------|
| [html](slides/09-1-trees.html)  [pdf](slides/09-1-trees_handout.pdf) | Slides | Tree-based methods
| [Rmd](notebooks/tree_splits.Rmd) | Notebook | Basics of tree algorithms |

*To be updated*

## Trees and forests

## Compositional nonlinearity

## (not active yet) Slides, notebooks, exercises

[Slides](slides/09-1-ensembles.html) for (tree) ensembles ([PDF])

[Slides](slides/09-2-composition.html) for deep learning ([PDF])

[Notebook](notebooks/tree_splits.html) for tree splitting

