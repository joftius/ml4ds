---
title: "Machine learning"
subtitle: "Additive univariate non-linearity"
author: "Joshua Loftus"
#institute: "LSE"
#date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "xaringan-themer.css", "../../../theme.css"]
#    seal: false    
    lib_dir: libs
    nature:
      titleSlideClass: ["bottom", "left"]
      countdown: 59000
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
---

class: inverse

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE)
options(knitr.table.format = "html")
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#2d708e",
  secondary_color = "#230433",
  link_color = "#55c667",
  text_bold_color = '#f68f46',
  title_slide_background_color = "#ffffff", #"#042333",
  title_slide_background_image = "../../../files/theme/LSE/spectra_close.jpg",
#    "https://upload.wikimedia.org/wikipedia/commons/1/1a/Workhouse_Nantwich.jpg",
  title_slide_background_size = "cover",
  ) #or contain
```

```{r xaringanextra, include=FALSE, warning=FALSE}
library(xaringanExtra)
#xaringanExtra::use_animate_all("slide_left")
xaringanExtra::use_tile_view()
xaringanExtra::use_panelset()
xaringanExtra::style_panelset_tabs(font_family = "inherit")
```

```{r tidyverse, include=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
theme_set(theme_minimal(base_size = 22))
set.seed(1)
library(broom)
library(modelr)
```

<style type="text/css">
.remark-slide-content {
    font-size: 1.2rem;
    padding: 1em 4em 1em 4em;
}
</style>

## Additive

separate non-linear terms are combined by addition

## univariate

each non-linear term uses only one predictor

## non-linearity

can be fit using various methods we've already learned

### GAM: **G**eneralized **A**dditive **M**odel

---

## Additive modeling assumption


- **Linearity** assumption: each predictor has a *coefficient*

$$g(\mathbb E[\mathbf y | \mathbf X]) = \beta_0 + \beta_1 \mathbf x_1 + \beta_2 \mathbf x_2 + \cdots + \beta_p \mathbf x_p$$
- **Additivity** assumption: each predictor has a *function*

$$g(\mathbb E[\mathbf y | \mathbf X]) = \beta_0 + f_1( \mathbf x_1) + f_2( \mathbf x_2) + \cdots + f_p(\mathbf x_p)$$

--

Includes linear models as special case if $f_j(\mathbf x_j) = \beta_j \mathbf x_j$

Assumptions / modeling choices:

- Assume $f_j$ is in some function space / fit with some method
- e.g. global polynomial, `loess`, local/kernel regression, smoothing splines, etc--pick your favorite!
- Can use same/different methods for each predictor


---

### Non-linear regression

Other times it's less clear, based on noise level and sample size


```{r nonlinear-reg-simple}
f1 <- function(x) -1 + 2*x - x^2
f2 <- function(x) sin(pi*x)
f3 <- function(x) exp(-5*(x-1/2)^2)

set.seed(1)
n <- 400
df <- data.frame(
  x1 = 2*(runif(n)-1/2),
  x2 = sample(1:100 / 50,  n, replace = TRUE),
  x3 = runif(n)
) %>%
  mutate(
    y = f1(x1) + f2(x2) + f3(x3) + rnorm(n) #<<
)
```

---

### Univariate plots

```{r}
uni_plot <- function(j) {
  xj <- paste0("x", j)
  fj <- paste0("f", j)
  ggplot(df, aes(get(xj), y)) +
    geom_point(alpha = .5) +
    geom_smooth() + xlab(xj) +
    geom_function(fun = get(fj),
                  size = 1,
                  color = "green") +
    theme(axis.text.x=element_blank(),
          axis.ticks.x=element_blank())
}
p1 <- uni_plot(1)
p2 <- uni_plot(2)
p3 <- uni_plot(3)
```

Side by side plots by adding with the `patchwork` library

---

```{r fig.width = 10, fig.height = 6, fig.align='center'}
library(patchwork)
p1 + p2 + p3
```

---

## Bias? Why? `r emo::ji("astonished")`

The true model is additive

We plot each variable separately but the `loess` curves are biased...

--

To fit $\hat f_1$ we would *ideally* do `loess` on

$$
y - f_2(\mathbf x_2) - f_3(\mathbf x_3)
$$

But we don't know $f_2$ and $f_3$, we are trying to estimate them too!

---

### **Backfitting** algorithm

1. Start with some initial estimates $\hat f_j$, e.g. from `y ~ x_j`

2. Iterate over $j$, updating $\hat f_j$ by fitting `r_j ~ x_j` where the partial residual $\mathbf r_j$
$$\mathbf r_j = \mathbf y - \hat \beta_0 - \sum_{k \neq j} \hat f_k(\mathbf x_k)$$
is computed using the current fits for all the other predictors

3. Repeat until "convergence" (some stopping rule)


---

### Remember the movie ratings data?

```{r}
library(ggplot2movies)
df <- movies %>% 
  filter(length <= 200, length > 10,
         year > 1918, votes >= 5) #, Short != 1)
```


I asked on [Twitter](https://twitter.com/tslumley/status/1361789344118284288) what was missing from the plot of movie length vs movie rating and Thomas Lumley suggested confounding by **year**

---

### One univariate non-linear relationship

```{r echo = F, fig.width = 10, fig.height = 7.5, fig.align='center'}
df %>%
  ggplot(aes(length, rating)) +
  geom_point(alpha = .1) + geom_smooth()
```



---

### Another univariate non-linear relationship

```{r echo = F, fig.width = 10, fig.height = 7.5, fig.align='center'}
df %>%
  ggplot(aes(year, rating)) +
  geom_point(alpha = .1) + geom_smooth()
```

---

### Additive combination of non-linear predictors


```{r}
library(gam)
fit_gam_loess <- 
  gam(rating ~ lo(length) + lo(year), data = df)
```

`lo` is for `loess`, but can use different methods too

```{r}
tidy(fit_gam_loess)
```
No coefficients, so how do we interpret?

---

### Replace each linear coefficient with 2d plot

```{r fig.width = 9, fig.height = 5.5, fig.align='center'}
par(mfrow = c(1,2))
plot(fit_gam_loess)
```

---

### Interpretation: holding other variables constant

```{r}
df_hat <- df %>% 
  mutate(.fitted = predict(fit_gam_loess))

df_fixed_year <- df_hat %>%
  filter(year %in% c(1950, 1960, 1970, 1980, 1990, 2000))

df_fixed_length <- df_hat %>%
  filter(length %in% c(80, 100, 120))
```

Let's look at a few specific years and plot the **fitted relationship** with length for each of those subsets of the data

Do the same for a few specific lengths and **fitted relationship** with year

---

### "Coefficient" of length, holding year constant

```{r fig.width = 9, fig.height = 5.5, fig.align='center'}
df_fixed_year %>%
  ggplot(aes(length, rating)) + geom_point(alpha = .1) +
  geom_line(aes(y = .fitted)) + facet_wrap(~ year)
```

---

### "Coefficient" of year, holding length constant

```{r fig.width = 9, fig.height = 5.5, fig.align='center'}
df_fixed_length %>%
  ggplot(aes(year, rating)) + geom_point(alpha = .1) +
  geom_line(aes(y = .fitted)) + facet_grid(~ length) + theme(axis.text.x=element_text(angle = 45)) + scale_x_continuous(breaks = c(1930, 1960, 1990))
```

---

### Choosing function spaces and methods


|    Goals   | $n > p$ (tall) | $n \approx p$ or $p > n$ (wide)    |
| :---        |    :----:   |          :---: |
| Prediction only      | Network methods       | Ridge   |
| + Interpretation   | See below        | Lasso      |

Additivity $\to$ GAMs. Interactions $\to$ tree methods

Reasoning for GAMs:

- Need larger $n$ to estimate non-linear relationships
- Typically fit GAMs with **smooth** non-linear functions
- Can include *pre-specified* interactions, but this changes interpretation (complicates partial derivatives/plots)

Non-smooth or many interactions $\to$ use trees instead

---

### Can additivity/GAMs be *importantly wrong*?

Interpretation: think carefully about **calculus** and **causality**. To simplify let's consider the identity link function (rather than e.g. logistic regression, those cases are more complicated)

#### Calculus

Does the CEF really decompose into additive terms?  Is this approximation good:

$$
\frac{\partial}{\partial x_j} \mathbb E[Y | \mathbf X] \approx g(x_j)
$$
Or does the relationship between the average of $Y$ and $x_j$ vary depending on the value of another predictor $x_k$?

---

### Can additivity/GAMs be *importantly wrong*?

Interpretation: think carefully about **calculus** and **causality**. To simplify let's consider the identity link function (rather than e.g. logistic regression, those cases are more complicated)

#### Causality

First, remember that causality is separate from prediction

But also, it may be a reason for doubting additivity

For example, if $X_k$ is a cause of $X_j$, or if they have a common cause, then we may want to include an interaction term for them

---

### Interactions in the movies data

Does the relationship between length and rating change depending on the year? Let's check a few years

```{r echo = F, fig.width = 10, fig.height = 6, fig.align='center'}
df_fixed_year %>%
  ggplot(aes(length, rating)) + geom_point(alpha = .1) +
  geom_smooth() + facet_wrap(~ year)
```

---

### Misspecification: failure of additivity

Difficult to tell because of small $n$ outside the range of length between 1 and 2 hours

But I think it's possible the *relationship* is changing over time, i.e. there is an interaction


$$
\frac{\partial}{\partial \text{length}} \mathbb E[\text{rating} | \text{length}, \text{year}] \approx g(\text{length}, \text{year})
$$

Since the right hand side does not depend on length *only*, the additive model might be a poor fit

Less accurate predictions

(Possibly importantly) wrong interpretations

---

### A hint of causality?

```{r echo = F, fig.width = 10, fig.height = 7.5, fig.align='center'}
df %>%
  ggplot(aes(year, length)) +
  geom_point(alpha = .2) + geom_smooth()
```

