---
title: "Exercise set 2"
author: Joshua Loftus
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
# install.packages("glmnet") #if necessary
library(glmnet) 
library(tidyverse)
library(knitr)
```

## High-dimensional regression

Solutions

#### Generate data (3 points)

```{r}
set.seed(1) # change this to some other number
# Generate a matrix of predictors X
# with 3 rows, 10 columns, and i.i.d. normal entries
p <- 10
n <- 3
X <- matrix(rnorm(n * p), nrow = n)
# Create a sparse coefficient vector beta
# with only 1 or 2 nonzero entries
beta <- rep(0, p)
beta[1] <- 1
# Compute outcome y from the noise-free linear model
y <- X %*% beta
```

#### Compute pseudoinverse (3 points)

Use output from the `svd` function to compute a right pseudoinverse of `X`. (Note: if you use a source aside from the Wikipedia articles above to figure out how to do this you should cite your source and include a link if it's a website)

```{r}
S <- svd(X)
X_pseudoinv <- S$v %*% diag(1/S$d) %*% t(S$u)
```

#### Verify right-inverse property (3 points)

```{r}
X %*% X_pseudoinv
```

**Explanation**: (1 point) Expected to be the identity matrix, and the diagonal entries are numerically close to 1 and off diagonal close to zero, i.e. the matrix is numerically close to the identity matrix.

#### Compare estimated beta to true beta (3 points)

```{r}
beta_hat <- X_pseudoinv %*% y
cbind(beta, beta_hat) |> kable()
```

**Explanation**: (1 point) Expected they are not equal, because we do not have enough data to exactly solve for or estimate beta

#### Compute MSE for predicting y (3 points)

```{r}
y_hat <- X %*% beta_hat
mean((y - y_hat)^2)
```

**Explanation**: (1 point) Expected to be numerically close to zero, because perfect prediction is possible when `p > n`.

#### Generate a new sample of test data and compute the (in-distribution) test MSE (3 points)

```{r}
X_test <- matrix(rnorm(n * p), nrow = n)
y_test <- X_test %*% beta
y_test_hat <- X_test %*% X_pseudoinv %*% y
mean((y_test_hat - y_test)^2)
```

**Explanation**: (1 point) Expected that this would be larger than zero, because test error is generally higher than training error.

#### Use penalized regression to estimate beta (4 points)

Compute the ridge and lasso estimates using `lambda = 0.1`, and compare these with the estimate from using `svd`. 

```{r}
beta_ridge <- coef(glmnet(X, y, intercept = FALSE, alpha = 0, lambda = .1))
beta_lasso <- coef(glmnet(X, y, intercept = FALSE, lambda = .1))
```


```{r}
# Comparison
cbind(beta, beta_lasso[-1], beta_ridge[-1], beta_hat) |>
  kable()
```

**Explanation**: (1 point) Expected lasso solution to be sparse and ridge solution to not be sparse.

#### Compute test MSE using penalized regression estimates (3 points)

```{r}
y_test_hat <- X_test %*% beta_lasso[-1]
mean((y_test_hat - y_test)^2)

y_test_hat <- X_test %*% beta_ridge[-1]
mean((y_test_hat - y_test)^2)
```

**Explanation**: (1 point) Expected or not, the lasso estimate is closest to the true beta and the test error is lower for the lasso compared to the other methods.

#### What are the first two variables to have nonzero coefficients in the lasso solution path as lambda decreases? (3 points)

(Note: be careful about whether there is an intercept estimate)

```{r}
lasso_path <- glmnet(X, y, intercept = FALSE)
# from this output I see Df is 1
# when lambda is 1 and 2 when lambda is .3
which(coef(lasso_path, s = 1)[-1] != 0)
which(coef(lasso_path, s = .2)[-1] != 0)
```
**Explanation**: (no points) The first two variables are X7 and then X1