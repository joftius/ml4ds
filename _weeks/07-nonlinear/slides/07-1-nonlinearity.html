<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Machine learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Joshua Loftus" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="../../../theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: bottom, left, title-slide

# Machine learning
## Non-linear supervised learning
### Joshua Loftus

---


class: inverse









&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 1.2rem;
    padding: 1em 4em 1em 4em;
}
&lt;/style&gt;

## Who's afraid of nonlinearity?

#### Simulated examples with pictures for intuition

#### What's different/difficult about nonlinear models

#### Concluding real data example

Upcoming lectures on variety of approaches

- Nearest neighbors
- Kernels
- Trees
- GAMs (Generalized Additive Models)
- Networks

Can use any for both **regression** and **classification**

---

# "Linear modeling **assumption**"

Why are we so often *assuming* linearity? (of the right hand side)

$$
g(\mathbb E[\mathbf y]) = \beta_0 + \beta^T \mathbf x
$$

- Easier to interpret, sure...
- But also easier to estimate

Sometimes non-linearity is clear from the data or domain info

Other times it's less clear, and makes it harder to learn a CEF


---

### Non-linear classification

Sometimes the relationships in the data are clearly nonlinear

.pull-left[
&lt;img src="07-1-nonlinearity_files/figure-html/circle-eg-1.png" width="500px" style="display: block; margin: auto;" /&gt;
]
.pull-right[
&lt;img src="07-1-nonlinearity_files/figure-html/poly-eg-1.png" width="500px" style="display: block; margin: auto;" /&gt;
]

- Linear decision boundary `\(\to\)` more/systematic errors
- Allow curving boundary `\(\to\)` lower error rate, not systematic

---

### Non-linear regression

Other times it's less clear, based on noise level and sample size

.pull-left[
&lt;img src="07-1-nonlinearity_files/figure-html/nonlinear-reg-simple-1.png" width="504" /&gt;
]
.pull-right[
&lt;img src="07-1-nonlinearity_files/figure-html/nonlinear-reg-sin-1.png" width="504" /&gt;

]

One CEF is `\(f(x) = -1 + 2x - x^2\)`, the other is `\(f(x) + g(x)\)`

---

### Fitting the "true" models

.pull-left[

```r
fit &lt;- function(D) {
  list(
  lm(y ~ x, D),
  lm(y ~ f(x), D),
  lm(y ~ f(x) + g(x), D)) 
}
models_data_f &lt;- 
  fit(data_f)
models_data_fg &lt;- 
  fit(data_fg)
```
Lists of fitted models on each dataset
- Linear (underfit?)
- `\(f(x)\)`
- `\(f(x) + g(x)\)`
]
.pull-right[

```r
models_data_f
```

```
## [[1]]
## 
## Call:
## lm(formula = y ~ x, data = D)
## 
## Coefficients:
## (Intercept)            x  
##       1.019       -1.053  
## 
## 
## [[2]]
## 
## Call:
## lm(formula = y ~ f(x), data = D)
## 
## Coefficients:
## (Intercept)         f(x)  
##   0.0009062    1.0027503  
## 
## 
## [[3]]
## 
## Call:
## lm(formula = y ~ f(x) + g(x), data = D)
## 
## Coefficients:
## (Intercept)         f(x)         g(x)  
##   0.0007602    1.0025218    0.0081946
```
]

---


```r
map_dfr(models_data_f, glance) # true CEF = f
```

```
## # A tibble: 3 x 12
##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     0.227         0.225  1.08     117.  4.20e-24     1  -598. 1201. 1213.
## 2     0.282         0.281  1.04     157.  1.54e-30     1  -583. 1172. 1184.
*## 3     0.282         0.279  1.04      78.1 2.42e-29     2  -583. 1174. 1190.
## # â€¦ with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;
```


```r
map_dfr(models_data_fg, glance) # CEF = f + g
```

```
## # A tibble: 3 x 12
##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     0.189         0.187  1.29      92.9 6.83e-20     1  -668. 1341. 1353.
## 2     0.221         0.219  1.26     113.  2.35e-23     1  -660. 1325. 1337.
*## 3     0.469         0.467  1.04     175.  2.52e-55     2  -583. 1174. 1190.
## # â€¦ with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;
```

Both look like high noise level, but 1 has ~double `\(R^2\)`? ðŸ¤¨ 

---

### Revealing `\(f(x) + g(x)\)` ðŸ¤ª 

.pull-left[
&lt;img src="07-1-nonlinearity_files/figure-html/unnamed-chunk-5-1.png" width="504" /&gt;
]
.pull-right[
&lt;img src="07-1-nonlinearity_files/figure-html/unnamed-chunk-6-1.png" width="504" /&gt;
]

Datasets *look* very similar, but `\(f+g\)` fits one and not the other


---

## If not linear, then what?

Choose a **space of functions** to optimize over

- Linear functions in `\(p\)` variables `\(\leftrightarrow\)` vector space `\(\mathbb R^p\)`

- Polynomials up to a fixed, maximum degree: also finite dimensional vector space

- Many (non-linear) function spaces are **infinite dimensional** vector spaces

  - `\(\{ f_k(x) = \sin(k \pi x) : k \in \mathbb Z \}\)` (Fourier basis)
  
  - Spaces of integrable functions, or differentiable

- Underlying math: linear algebra `\(\to\)` functional analysis

---

### Intuitions about function spaces

- Optimize over a larger space `\(\to\)` fit more complex models

- Bias-variance trade-off: *both* choice of right/good space of functions *and* amount of complexity in that space

  - e.g. periodic (like last example), right wavelengths
  
  - e.g. smooth, right amount of wiggliness
  
  - e.g. "Shape constraints" like monotonic, unimodal, (log-)concave (*Application*: epidemic trajectory)

Science/modeling/inference approach: domain knowledge, first principles

ML approach: whichever function space has current SOTA software (with easy to use default settings ðŸ˜†)

---

### Optimizing over a large function space


```r
overfit &lt;- function(D, k_range = 0:200) {
  fit_sin_k &lt;- function(k) {
    fit_k &lt;- lm(y ~ x + sin(k*x), data = D)
    glance(fit_k)$r.squared
  }
  r_squareds &lt;- map_dbl(k_range, fit_sin_k)
  best_k &lt;- k_range[which.max(r_squareds)]
  best_k
}
khat_f &lt;- overfit(data_f)
khat_fg &lt;- overfit(data_fg)
c(khat_f, khat_fg)
```

```
## [1]   1 100
```

$$
\hat f(x) = \beta_0 + \beta_1 x + \beta_2 \sin(\hat k x)
$$

Apparently `\(\hat k = 1\)` or `\(\hat k = 100\)`, respectively

---

### Plotting the "best" models



.pull-left[
&lt;img src="07-1-nonlinearity_files/figure-html/unnamed-chunk-9-1.png" width="504" /&gt;
]
.pull-right[
&lt;img src="07-1-nonlinearity_files/figure-html/unnamed-chunk-10-1.png" width="504" /&gt;
]

Can we believe this?

---

### So which is it?

When we aren't doing simulations we just have the data

.pull-left[
&lt;img src="07-1-nonlinearity_files/figure-html/unnamed-chunk-11-1.png" width="504" /&gt;
]
.pull-right[
&lt;img src="07-1-nonlinearity_files/figure-html/unnamed-chunk-12-1.png" width="504" /&gt;
]

We don't know signal/noise level, function space, complexity...

---

### The "big data" advantange

With larger samples we could tell these two cases apart

.pull-left[
&lt;img src="07-1-nonlinearity_files/figure-html/unnamed-chunk-13-1.png" width="504" /&gt;
]
.pull-right[
&lt;img src="07-1-nonlinearity_files/figure-html/unnamed-chunk-14-1.png" width="504" /&gt;
]

Use more data for validation / in-distribution generalization

---

## Non-linearity and overfitting

Much of machine learning and "AI" is about having large enough datasets to search large spaces of functions and fit complex models without **variability problems** from overfitting

i.e. good in-distribution generalization (new data, same DGP)

#### Intuition: more complex models are more sensitive to small changes in the data, or more "brittle"

--

#### Statistical wisdom: another reason to prefer simpler models may be better out-of-distribution generalization

i.e. avoiding **bias problems** from overfitting

---

### Out-of-distribution generalization

What if we test on data outside the original range/distribution?

.pull-left[
&lt;img src="07-1-nonlinearity_files/figure-html/unnamed-chunk-15-1.png" width="504" /&gt;
]
.pull-right[
&lt;img src="07-1-nonlinearity_files/figure-html/unnamed-chunk-16-1.png" width="504" /&gt;
]

Simpler/"underfit" models (dashed lines) *might* do better

---

### Choosing function spaces and methods

Since this is a course in ML, we won't assume these choices can be informed by domain knowledge

A few examples based on high level **properties of the data** and **goals of the analysis** -- not an exhaustive list or flowchart

(Assuming data shape is rectangular and i.i.d., otherwise we need specialized models for other data/dependence types)

|    Goals   | `\(n &gt; p\)` (tall) | `\(n \approx p\)` or `\(p &gt; n\)` (wide)    |
| :---        |    :----:   |          :---: |
| Prediction only      | Network methods       | Ridge   |
| + Interpretation   | See below        | Lasso      |

Additivity `\(\to\)` GAMs. Interactions `\(\to\)` tree methods

---

## Caveats 

- Nearest neighbors: more often treated as an uninteresting baseline for comparison

- Some methods can be combined, all can be customized, more "researcher degrees of freedom" -- and therefore more model complexity -- if there are more ways to customize (e.g. network models)

- Remember: "[All models are wrong](https://en.wikipedia.org/wiki/All_models_are_wrong) but some are useful"

- Remember: [No Free Lunch](https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization) -- methods that do well in one situation will do poorly in another

Learning about a diverse set of approaches helps our understanding, and you will have more than [one kind of tool](https://en.wiktionary.org/wiki/if_all_you_have_is_a_hammer,_everything_looks_like_a_nail) at your disposal

---

### Smooth non-linearity?

CEF based on bins (like nearest neighbors). Credit: Reddit user [SurfingPolice](https://www.reddit.com/r/dataisbeautiful/comments/lkjlhy/relationship_between_film_length_and_average_imdb/gnk46yh/)



&lt;img src="07-1-nonlinearity_files/figure-html/unnamed-chunk-18-1.png" width="504" /&gt;

---
  
### Disaggregated (full data instead of bin averages)
  
&lt;img src="07-1-nonlinearity_files/figure-html/unnamed-chunk-19-1.png" width="504" /&gt;

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"highlightSpans": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
(function(time) {
  var d2 = function(number) {
    return ('0' + number).slice(-2); // left-pad 0 to minutes/seconds
  },

  time_format = function(total) {
    var secs = Math.abs(total) / 1000;
    var h = Math.floor(secs / 3600);
    var m = Math.floor(secs % 3600 / 60);
    var s = Math.round(secs % 60);
    var res = d2(m) + ':' + d2(s);
    if (h > 0) res = h + ':' + res;
    return res;  // [hh:]mm:ss
  },

  slide_number_div = function(i) {
    return document.getElementsByClassName('remark-slide-number').item(i);
  },

  current_page_number = function(i) {
    return slide_number_div(i).firstChild.textContent;  // text "i / N"
  };

  var timer = document.createElement('span'); timer.id = 'slide-time-left';
  var time_left = time, k = slideshow.getCurrentSlideIndex(),
      last_page_number = current_page_number(k);

  setInterval(function() {
    time_left = time_left - 1000;
    timer.innerHTML = ' ' + time_format(time_left);
    if (time_left < 0) timer.style.color = 'red';
  }, 1000);

  slide_number_div(k).appendChild(timer);

  slideshow.on('showSlide', function(slide) {
    var i = slide.getSlideIndex(), n = current_page_number(i);
    // reset timer when a new slide is shown and the page number is changed
    if (last_page_number !== n) {
      time_left = time; last_page_number = n;
      timer.innerHTML = ' ' + time_format(time); timer.style.color = null;
    }
    slide_number_div(i).appendChild(timer);
  });
})(59000);
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
