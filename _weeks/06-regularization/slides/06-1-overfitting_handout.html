<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Machine learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Joshua Loftus" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/animate.css-xaringan/animate.slide_left.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: middle, center









&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 1.2rem;
    padding: 1em 4em 1em 4em;
}
&lt;/style&gt;

# What *is* overfitting?

### You've probably heard about it

### I've delayed it intentionally

I want to do justice to this extremely important, central concept of machine learning

And I'm dissatisfied with the usual definitions!

---
class: center, middle

# Machine learning

## is about algorithms

### that allow us to increase model complexity

and optimize

on larger datasets

over larger sets of parameters

and even infinite-dimensional function spaces

---

### With great fitting comes great responsibility

ML increases danger of this specific kind of modeling problem

.pull-left[
![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/600px-Overfitting.svg.png)
Source: [Wikipedia](https://en.wikipedia.org/wiki/Overfitting)
]
.pull-right[
![](https://joshualoftus.com/ml4ds/weeks/01-introduction-foundations/slides/01-2-foundations_files/figure-html/gapminder-loess-1.png)
Example in week 1
]


---

## Outline

- **Complexity**: typical machine learning definition

- **Generalization**: simple probabilistic definition

- **Bias-variance**: statistical distinctions

- **Causality**: scientific/philosophical applications

- **Anthropology**: human learning and overfitting IRL (*supplemental but brief and useful for life in general?*)

- **Validation**: standard method to prevent overfitting (*to variation*, cannot help us with bias)

---
class: center, middle

# Typical ML definition of overfitting

Motivating idea: assume the model will be "deployed"

I.e. Some time after fitting the model will be used on new data

&gt; "It is difficult to make predictions, especially about the future" - [Danish saying](https://quoteinvestigator.com/tag/niels-bohr/)

---

# Overfitting the "training data"

Using a model that is *too complex*

Specifically, one where the complexity is larger than the optimal complexity for predicting on a *new observation* or *new sample* of **test/validation data**

- `\(\hat f_\lambda\)` model fitted/estimated on **training data**

- `\(\lambda\)` tuning parameter that *penalizes* complexity
  - larger `\(\lambda\)`, simpler model

- `\(\lambda^*\)` optimal param. value for predicting/classifying *new data*

- Overfitting: using `\(\hat f_\lambda\)` for some `\(\lambda &lt; \lambda^*\)`

---

## ISLR Figure 2.10

.pull-left[
True model is simple
]
.pull-right[
High complexity overfits
]

![](islr2.10.png)

---


## ISLR Figure 2.11


.pull-left[
True model is complex
]
.pull-right[
Low complexity underfits
]

![](islr2.11.png)


---

# The end

Most discussions of overfitting end there

Some go on a little more, relating it to **bias-variance** trade-off

Overfitting: *low bias but overwhelmingly high-variance*

(we'll do that soon)


---
class: center, middle

# Generalization

and a

## probabilistic definition

Motivation: what is the *probability distribution* of the test data?

---

# Two kinds of generalization

ML/AI books/courses talk about "generalization error"

Over-used term, same word / *importantly different* meanings

### Generalization to a new observation from...

- the same distribution or DGP
- a different (but related) distribution

and *corresponding reasons for doing poorly* 

- variance ("random/unstructured error", high entropy)
- bias ("systematic/structured error", low entropy)

---

### Think about distributions

Suppose the training data is sampled i.i.d. from
`\((\mathbf X_1, y_1), (\mathbf X_2, y_2), \ldots, (\mathbf X_n, y_n) \sim F\)`

and the test data is sampled i.i.id from `\((\mathbf X'_1, y'_1), (\mathbf X'_2, y'_2), \ldots, (\mathbf X'_{n'}, y'_{n'}) \sim F'\)`

#### In-distribution (ID) generalization: `\(F = F'\)`

Under/overfitting, **variability problem**, larger `\(n\)` allows more complex models to be fit

#### Out-of-distribution (OOD) generalization: `\(F \neq F'\)`

"Covariate/distribution/dataset shift", **bias problem**, larger `\(n\)` may not help. Need **modeling assumptions** like `\(F' \approx F\)`

---

## Optimism and ID generalization

Observation: training error generally appears lower than test/validation error. Why?

Risk vs *empirical* risk minimization

`$$R(g) = \mathbb E_F[L(\mathbf X,Y,g)]$$`

`$$\hat f =  \arg \min_g \hat R(g) = \arg \min_g \frac{1}{n} \sum_{i=1}^n L(\mathbf x_i, y_i, g)$$`

**Fact**: for some `\(\text{df}(\hat f) &gt; 0\)` (depends on problem/fun. class)

`$$\mathbb E_{Y|\mathbf x_1, \ldots, \mathbf x_n}[R(\hat f) - \hat R(\hat f)] = \frac{2\sigma^2_\varepsilon}{n} \text{df}(\hat f) &gt; 0$$`

---

### Optimism, ID gen., and degrees of freedom

#### Linear case

If `\(\hat f\)` is linear with `\(p\)` predictors (or `\(p\)` basis function transformations of original predictors) then

$$
\text{df}(\hat f) = p
$$

#### Fairly general case

For many ML tasks and fitting procedures

`$$\text{df}(\hat f) \text{ increases as } \frac{1}{n \sigma^2_\varepsilon} \sum_{i=1}^n \text{Cov}(\hat f (\mathbf x_i), y_i) \text{ increases}$$`

---

### Take-aways about optimism and ID gen.

- Empirical risk *underestimates* actual risk (ID generalization error)

- The magnitude of this bias is called **optimism**

- Optimism generally increases with function class complexity

  - e.g. for linear functions, increases linearly in `\(p\)`

- For a fixed function class, optimism decreases linearly in `\(n\)`

- Too much optimization `\(\to\)` overfitting `\(\to\)` more optimism

---

## Two kinds of overfitting?

Many sources identify overfitting as a threat to generalization

Typically only apply this to **ID generalization**, and have solution strategies to avoid the **variability problems** due to overfitting

#### But overfitting is also a threat to **OOD generalization**! 

This kind of generalization is often what we practically want

There are serious **bias problems** due to overfitting

#### Let's start using new terminology

*Overfitting to variation* and *overfitting to bias*

---

### Now let's jump from probability to statistics

And talk about why we always need to care about *both kinds of generalization*

---
class: center, middle

# Statistical aspects of overfitting

Motivation: all models are wrong, including `\(F\)` and `\(F'\)`

or

Motivation: overfitting to noise... what's noise?

---

# What *is* noise?

The effect of all (causal?) factors not captured by the model

Could be different reasons for failing to capture

- Measurement issues
- Wrong functional relationship
- Variables excluded (maybe not even measured or defined)

*Does not require physical randomness* (which maybe doesn't exist...)

Something considered **noise** in one setting, or by one modeler, could be **signal** to a different observer

---

### Noise and residuals in regression

(*One of the most "agnostic" or minimal-theory ways of defining regression is as estimation of a conditional expectation function, without assuming any specific functional form like linearity*). The "noise" in regression is defined as

$$
\varepsilon = y - \mathbb E_F[y | \mathbf x]
$$

But this is math, not applied data analysis! Requires assuming a probability distribution `\(F\)` / random variable model

Otherwise, how do we define expectation?

We never observe `\(\varepsilon\)`, only residuals `\(r_i = y_i - \hat f(\mathbf x_i)\)` of some model `\(\hat f\)` *fit with specific assumptions/algorithms*

---

# What is bias/variability?

Two analysts start with different assumptions

e.g. linearity vs flexible non-parametric methods

Fit different regression functions

Compute different residuals

See different patterns (or lack thereof) in residuals

Something considered **variation** in one setting, or by one modeler, could be **bias** to a different observer

---

## Data science

In the "real world" there is a data generation process (DGP)

We *assume* this can be modeled as an i.i.d. sample from a probability distribution `\(F\)`

Probability model / mathematical justification for our methods

#### All models are wrong

Could model DGP as a mixture of distributions `\(F\)` and `\(F'\)` (heterogeneity), or time-varying `\(F^t\)`

Training/test data randomly shuffled?

Generalization in/out of distribution?

---

## Two data scientists diverged...

Starting with different assumptions about DGP

Use different strategies to avoid overfitting

e.g. different ways of splitting into training and test data

Something considered **ID generalization** in one setting, or by one modeler, could be **OOD generalization** to a different observer

---

## Statistical take-aways

Mathematical distinctions between ID and OOD generalization rely on assumptions (as do statistical distinctions between bias and variability)

ML methods for avoiding overfitting are motivated by ID generalization, guard against **overfitting to variability**

In applications, ID/OOD distinctions break down. If we probe them a bit we find it's more gray area / ambiguous 

Most scientists and decision-makers care about [external validity](https://en.wikipedia.org/wiki/External_validity), conceptually related to OOD generalization

**Overfitting to bias** is a serious, widely neglected problem!

---
class: center, middle

# Considerations

of the 

## scientific and philosophic variety

### with respect to overfitting

Motivation: does science overfit? Can philosophy of science help us understand how to prevent it? What about causality?

---

### Stability, invariance, and causality

**Idea**: causal relationships should persist despite (some) [changes](https://plato.stanford.edu/entries/causation-counterfactual/#ConInv) in "[background](https://static1.squarespace.com/static/54023011e4b00d99c7bb97f9/t/547b5b1fe4b0464e56d755b4/1417370399799/PoS1997.pdf) [conditions](https://static1.squarespace.com/static/54023011e4b00d99c7bb97f9/t/5445ceb4e4b06ae793389dad/1413861044101/BandP2010.pdf)"

[Bradford Hill criteria](https://en.wikipedia.org/wiki/Bradford_Hill_criteria) for causation

&gt; **Consistency**: Has [the observed association] been repeatedly observed by different persons, in different places, circumstances and times? 

Apparently [people think about causality](https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12605) this way

Can use the idea to motivate [statistical methods](https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssb.12167) for causal inference

---

### Overfitting as a threat to causal inference

[Bradford Hill criteria](https://en.wikipedia.org/wiki/Bradford_Hill_criteria) for causation

&gt; "the larger the association, the more likely that it is causal." - Wikipedia, not Hill

Hill:

&gt; the death rate from cancer of the lung in cigarette smokers is nine to ten times the rate in non-smokers

**Problem**: overfitting can make associations appear stronger

e.g. proportion of variation in `lifeExp` explained by `gdpPercap`

Increase flexibility, explain higher proportion... stronger evidence of causality? ðŸ¤”

---

### Generalization, novelty, and severity

[Philosophy of science](https://plato.stanford.edu/entries/prediction-accommodation/): prediction vs "accommodation"

Prediction: happens in time before observation/measurement

Accommodation: theory built to explain past observation/data

Accurate prediction is better evidence in favor of a scientific theory than mere accommodation

ML: What's better evidence in favor of the model?

Popper and Lakatos: **temporal novelty**

Zahar, Gardner, Worrall: **use-novelty** (or problem novelty)

[Mayo](http://bactra.org/reviews/error/): novelty is not necessary. **Severity** is necessary

---
class: center, middle

# Anthropology?

## ie. overfitting IRL

(in real life)

Motivation: do *we* overfit? ("Are we the baddies?")

Disclaimer: I am not an anthropologist *or* self-help author

---

## How/why are humans different?

We seem to better at *learning* than other animals

[Human eyes are different](https://en.wikipedia.org/wiki/Cooperative_eye_hypothesis), allowing us to see where others are looking

### Social learning

#### "Monkey see, monkey do"

Lots of animals learn by [imitation](https://en.wikipedia.org/wiki/Imitation#Over-imitation), but [humans specifically](https://psycnet.apa.org/fulltext/1993-44247-001.pdf) take imitation to a [whole](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0159920) [different](https://www.pnas.org/content/104/50/19751) level

#### Over-imitation, causal opacity, cultural evolution...


---
class: center, middle


# Validation

## Estimate test error directly

### using "**validation data**" / "**test data**"

#### i.e. a new set of data, "unseen" by `\(\hat f\)`

Indep. samples `\(D = \{  (\mathbf x_i, y_i) \}_{i=1}^n\)` and `\(D' = \{ (\mathbf x_i', y_i') \}_{i=1}^{n'}\)`

Estimate `\(\hat f\)` on `\(D\)`, evaluate `\(\hat f\)` on `\(D'\)`

---

## Motives

- Debiasing risk estimate. Since `\(\hat f\)` does not depend on `\(D'\)`, it is not **overfit to the variability** in `\(D'\)`

- If `\(\hat f\)` is overfit to `\(D\)` then its test error on `\(D'\)` will be large (complexity too high, variability too high)

- Actual practice: analogous to "deploying an ML model in production"

- Philosophy of science: use novelty, actual prediction (not accommodation)

- Tukey: [Exploratory Data Analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis) vs Confirmatory

- Use test error to choose **model complexity** / **amount of regularization**

---

# Choosing model complexity

## Using test/validation data

Indep. samples `\(D = \{  (\mathbf x_i, y_i) \}_{i=1}^n\)` and `\(D' = \{ (\mathbf x_i', y_i') \}_{i=1}^{n'}\)`

- Estimate `\(\hat f_\lambda\)` on `\(D\)` for a "path" or grid of `\(\lambda\)` values

- Evaluate `\(\hat f_\lambda\)` on `\(D'\)` and choose `\(\hat \lambda\)` accordingly (e.g. with minimum loss)

- Refit `\(\hat f_{\hat \lambda}\)` on full data `\(D \cup D'\)`, this is our final model

*Common when computational cost of fitting one model is high*

---

## Cross-validation

*When computational cost of fitting one model is not too high*

**Idea**: swap `\(D\)` and `\(D'\)` in previous process and get two estimates, `\(\hat R(\hat f_\lambda)\)` and `\(\hat R(\hat f_\lambda')\)`

Average these and choose `\(\hat \lambda\)` using the average (e.g. minimizer)

**Idea**: apply the same process with multiple independent "folds" of data

#### `\(K\)`-fold cross-validation

Each subset used once as test set, and `\(K-1\)` times for training

Minimize `\(\hat R_{K\text{-cv}}(\lambda) = \frac{1}{K} \sum_{k=1}^K \hat R_k(\hat f^{(k)}_\lambda)\)`

---

## Cross-validation cartoon

![](cv.wikipedia.svg.png)

Gives `\(K\)` estimates of test error (risk) at each `\(\lambda\)`

Credit: [Wikipedia](https://en.wikipedia.org/wiki/Cross-validation)

---

## `\(K\)`-fold cross-validation

Each subset used once as test set, and `\(K-1\)` times for training

Choose `\(\hat \lambda\)` to minimize

`$$\hat R_{K\text{-cv}}(\lambda) = \frac{1}{K} \sum_{k=1}^K \hat R_k(\hat f^{(k)}_\lambda)$$`
where `\(\hat f^{(k)}_\lambda\)` is fit on the dataset that "holds out" the `\(k\)`th fold

Then refit model `\(\hat f_{\hat \lambda}\)` at that value of `\(\hat \lambda\)` on the entire dataset

---

### Lessons about cross-validation

- Think of it as a way to **choose model complexity**

- **Beware** common cross-validation errors! From Duda and Hart quoted in [MLstory](https://mlstory.org/data.html)

&gt; ... the same data were often used for designing and testing the classifier. This mistake is frequently referred to as "testing on the training data." A related but less obvious problem arises *when a classifier undergoes a long series of refinements guided by the results of repeated testing on the same data. This form of "**training on the testing data**" often escapes attention until new test samples are obtained*.

---

### Lessons about cross-validation

- **Beware** common cross-validation errors! From ESL:

&gt; Ideally, the test set should be kept in a "vault," and be brought out only at the end of the data analysis. *Suppose instead that we use the test-set repeatedly, choosing the model with smallest test-set error. Then the test set error of the final chosen model will underestimate the true test error*, sometimes substantially.

- Cross-validate entire model building pipeline (not just one step), and only do it once -- or at *least* not many times

- Choosing `\(K\)`: larger `\(\to\)` `\(\hat R_{K\text{-cv}}\)` has lower bias, more variance. Often use `\(K = 5\)` or `\(10\)`

---

### Regularization

- Fancy sounding word for "simplification," simpler models
- Increases bias to reduce variance

### Cross-validation

- Fit and evaluate models on different subsets of data
- Choose amount of regularization/complexity
- Re-using data *more than once* `\(\to\)` overfitting again

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"highlightSpans": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
(function(time) {
  var d2 = function(number) {
    return ('0' + number).slice(-2); // left-pad 0 to minutes/seconds
  },

  time_format = function(total) {
    var secs = Math.abs(total) / 1000;
    var h = Math.floor(secs / 3600);
    var m = Math.floor(secs % 3600 / 60);
    var s = Math.round(secs % 60);
    var res = d2(m) + ':' + d2(s);
    if (h > 0) res = h + ':' + res;
    return res;  // [hh:]mm:ss
  },

  slide_number_div = function(i) {
    return document.getElementsByClassName('remark-slide-number').item(i);
  },

  current_page_number = function(i) {
    return slide_number_div(i).firstChild.textContent;  // text "i / N"
  };

  var timer = document.createElement('span'); timer.id = 'slide-time-left';
  var time_left = time, k = slideshow.getCurrentSlideIndex(),
      last_page_number = current_page_number(k);

  setInterval(function() {
    time_left = time_left - 1000;
    timer.innerHTML = ' ' + time_format(time_left);
    if (time_left < 0) timer.style.color = 'red';
  }, 1000);

  slide_number_div(k).appendChild(timer);

  slideshow.on('showSlide', function(slide) {
    var i = slide.getSlideIndex(), n = current_page_number(i);
    // reset timer when a new slide is shown and the page number is changed
    if (last_page_number !== n) {
      time_left = time; last_page_number = n;
      timer.innerHTML = ' ' + time_format(time); timer.style.color = null;
    }
    slide_number_div(i).appendChild(timer);
  });
})(59000);
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
