<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Machine learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Joshua Loftus" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="../../../theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: top, left, title-slide

# Machine learning
## Regularized regression
### Joshua Loftus

---


class: inverse









&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 1.2rem;
    padding: 1em 4em 1em 4em;
}
&lt;/style&gt;

### Benefits of bias

In "high-dimensions" (p &gt; 2)

### Sparse (interpretable?) regression

With the lasso

### Regularization

Tuning `\(\lambda\)` with cross-validation

### Inference

Is my high-dimensional model any good?

---
class: inverse, center, middle

# Bias *can* be good, actually

### Especially in higher dimensions

---

### Stein "paradox" and bias

Consider estimating `\(\mu\)` from
$$
Y_1, \ldots, Y_n \sim N(\mu, \sigma^2 I)
$$
The MLE is `\(\bar Y\)`

The JS estimator shrinks `\(\bar Y\)` toward some other point


```r
JS_MSE &lt;- function(mu, Z, n, p) {
  Y &lt;- replicate(n, rnorm(p, mean = mu))
  MLE &lt;- rowMeans(Y)
  S &lt;- mean(rowMeans((Y - MLE)^2)^2)
  shrinkage &lt;- max(0, 1 - (p-3) * (S/(n+1)) / sum((MLE-Z)^2))
  if (shrinkage == 0) print("yes")
  JS &lt;- Z + (MLE-Z) * shrinkage
  cbind((JS - mu)^2, (MLE - mu)^2)
}
```

---

## James-Stein estimator simulation

Generate and fix `\(\mu\)` and `\(\mathbb z\)`, then simulate many instances


```r
JS_MC &lt;- function(n, p, instances = 10000) {
  mu &lt;- rnorm(p) * rpois(p, 3)
  Z &lt;- rnorm(p)
  output &lt;- replicate(instances, JS_MSE(mu, Z, n, p))
  colMeans(apply(output, 2, rowMeans))
}
```

---

### High dimensional? Well, `\(p &gt; 2\)`


```r
set.seed(42) # reproducibility
```


```r
JS_MC(n = 5, p = 4)
```

```
## [1] 0.2015885 0.2023784
```

```r
JS_MC(n = 5, p = 4)
```

```
## [1] 0.1977813 0.1985183
```


```r
JS_MC(n = 50, p = 4) # larger sample size
```

```
## [1] 0.01987565 0.01989919
```

```r
JS_MC(n = 50, p = 4)
```

```
## [1] 0.01968495 0.01969554
```

---

### Higher-dimensional, now `\(p &gt; n\)`


```r
JS_MC(n = 2, p = 1000)
```

```
## [1] 0.4881896 0.5002847
```

```r
JS_MC(n = 2, p = 1000)
```

```
## [1] 0.4858198 0.5002442
```


```r
JS_MC(n = 20, p = 100) # larger sample size
```

```
## [1] 0.04986082 0.04999878
```

```r
JS_MC(n = 20, p = 100)
```

```
## [1] 0.04969324 0.04988654
```

---

### High-dimensional regression

Estimating parameters in a covariate space rather than the outcome space

Examples with `\(n &gt; p\)` so we can compare OLS to ridge


```r
set.seed(42)
instance &lt;- function(X, H, HL, n, p, beta, mu) {
  Y &lt;- mu + rnorm(n)
  y &lt;- Y - mean(Y)
  beta_hat_lm &lt;- H %*% y
  beta_hat &lt;- HL %*% y
  cbind((beta_hat - beta)^2, (beta_hat_lm - beta)^2)
}
```

---

### High-dimensional regression

Fix `\(X\)` and `\(\beta\)`, hence `\(\mu = X \beta\)`, but resample `\(Y\)`


```r
high_dim_MSE_MC &lt;- function(n, p, instances = 1000) {
  beta &lt;- rnorm(p) * rpois(p, 3)  
  X &lt;- matrix(rnorm(n*p), nrow = n)
  mu &lt;- X %*% beta
  XX &lt;- t(X) %*% X
  H &lt;- MASS::ginv(XX) %*% t(X)
  L &lt;- sqrt(2*log(p)/n)/2
  HL &lt;- MASS::ginv(XX + diag(L, p)) %*% t(X)
  output &lt;- replicate(instances,
                      instance(X, H, HL, n, p, beta, mu))
  colMeans(apply(output, 2, rowMeans))
}
```

---

### MSE(ridge) vs MSE(OLS)



```r
high_dim_MSE_MC(n = 100, p = 10)
```

```
## [1] 0.01143915 0.01145554
```


```r
high_dim_MSE_MC(n = 100, p = 90)
```

```
## [1] 1.977403 2.738305
```

```r
high_dim_MSE_MC(n = 100, p = 90)
```

```
## [1] 0.903344 1.439779
```


```r
high_dim_MSE_MC(n = 1000, p = 990, instances = 2000)
```

```
## [1] 0.09942627 0.19938088
```

```r
high_dim_MSE_MC(n = 1000, p = 990, instances = 2000)
```

```
## [1] 0.5808623 0.8106186
```

---

### Lessons about bias

#### Bias can help

- Even in such a basic problem as **estimating the multivariate normal mean** (JS)
- More important in higher dimensions

--

#### But it depends! Task-specific

  - What's the scientific question?
  - Which estimator(s) are we evaluating?
  - How will the estimator / ML pipeline be used? For what?

e.g. If `\(\hat \sigma\)` underestimates `\(\sigma\)` it may have lower MSE, but do we care about estimating `\(\sigma\)`? Or do we care about C.I. coverage?

---
class: inverse, center, middle


# Interpretable

## high-dimensional regression

### with the lasso

---

### Lasso vs ridge


```r
library(glmnet)
library(plotmo) # for plot_glmnet
n &lt;- 100
p &lt;- 20
X &lt;- matrix(rnorm(n*p), nrow = n)
beta = sample(c(-1,0,0,0,1), p, replace = TRUE)
Y &lt;- X %*% beta + rnorm(n)
lasso_fit &lt;- glmnet(X, Y)
ridge_fit &lt;- glmnet(X, Y, alpha = 0)
which(beta != 0)
```

```
## [1]  1  5  7 12 13 15
```

---

### Lasso vs ridge

.pull-left[

```r
plot(ridge_fit)
```

&lt;img src="06-1-regularization-phd_files/figure-html/unnamed-chunk-7-1.png" width="504" /&gt;
]
.pull-right[

```r
plot_glmnet(lasso_fit, xvar = "norm")
```

&lt;img src="06-1-regularization-phd_files/figure-html/unnamed-chunk-8-1.png" width="504" /&gt;

```
## [1]  1  5  7 12 13 15
```
]



---

### Lasso optimization problem

A simple `diff` to remember lasso/ridge is via the penalty/constraint (1-norm instead of 2-norm). Lasso is

`$$\text{minimize } \frac{1}{2n}\| \mathbf y - \mathbf X \beta \|_2^2 \text{ s. t. } \| \beta \|_1 \leq t$$`
where

`$$\| \beta \|_1 = \sum_{j=1}^p |\beta_j|$$`
Lagrangian form

`$$\text{minimize } \frac{1}{2n} \| \mathbf y - \mathbf X \beta \|_2^2 + \lambda \| \beta \|_1$$`
---

## Sparsity of lasso estimators

The `\(L^1\)` ball in `\(\mathbb R^p\)` `\(\{ x : \| x \|_1 = \text{const} \}\)` contains

- `\(2p\)` points that are 1-sparse `\(x_j = \pm 1\)`, `\(x_{-j} = 0\)`

- `\(\binom{p}{k} 2^k\)` points `\(k\)`-sparse with elements `\(\pm k^{-1}\)`

- Edges connecting (many, not all) pairs of these points

- Higher dimensional "faces" spanning (some) sets of these points, etc

--

The ellipsoid `\(\| \mathbf y - \mathbf X \beta \|_2^2 = \text{const}\)` *probably* intersects one of these...

---

## Sparsity of lasso estimators

At the point of intersection of the ellipse and the `\(L^1\)` ball, the normal vector of the ellipse has to be in the *normal cone* of the `\(L^1\)` ball (at the same point)

--

These normal cones are larger (in dimension) at sparser points on the ball (hence more likely to contain the normal vector of the ellipse)


- Normal cones are 1-dimensional on any of the `\((p-1)\)`-dimensional (least sparse) faces

- Normal cones are `\(p\)`-dimensional at the 1-sparse points (most sparse)

**Exercise**: Prove these two cases (start with a simple def. of normal cone, solve `\(p = 2\)` or 3 first, etc)

---

## KKT optimality conditions

Constrained optimization conditions usually taught in multivariate calculus

- Switch to lagrangian form
- Check stationary points (vanishing gradient)
- Check boundary/singularity points
- Verify feasibility (constraints satisfied)

The [Karush-Kuhn-Tucker](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions) (KKT) conditions generalize these

Very useful for analysis of constrained optimization problems (i.e. almost all ML methods)

---

## Stationary points

Of the least squares objective (uncorrelated residuals)

$$
\frac{1}{n} \mathbf X^T ( \mathbf X \hat \beta - \mathbf y) = 0
$$

For ridge (larger `\(|\hat \beta_j|\)` have larger resid. covariance)

$$
\frac{1}{n} \mathbf X^T ( \mathbf X \hat \beta - \mathbf y) = - 2\lambda \hat \beta
$$

For lasso (resid. |covar| = `\(\lambda\)` if `\(\hat \beta_j \neq 0\)` and `\(\leq \lambda\)` otherwise)

$$
\frac{1}{n} \mathbf X^T ( \mathbf X \hat \beta - \mathbf y) = - \lambda \text{ sign}(\hat \beta)
$$
*Lasso treats predictors more "democratically"*

---

## `\(L^1\)` subgradient = sign

**Exercise**:

Verify that the set-valued sign `\(s = \text{ sign}(\beta)\)`, defined coordinate-wise as

`$$s_j = \begin{cases} 1 &amp; \text{ if } \beta_j &gt; 0 \\ -1 &amp; \text{ if } \beta_j &lt; 0 \\ [-1, 1] &amp; \text{ if } \beta_j = 0 \end{cases}$$`

is the subgradient of the `\(L^1\)` norm function

(part of the point of the exercise is to "recall" the definition of a subgradient!)

---

## Interpreting "interpretable"

Usual linear model interpretation of coefficients

**If** the conditional expectation function (CEF) is *linear*
`$$f(\mathbb x) = \mathbb E[\mathbf Y | \mathbf X = \mathbf x] = \beta_0 + \sum_{j=1}^p \beta_j x_j$$`

Then

`$$\hat \beta_j \approx \frac{\partial}{\partial x_j} \mathbb E[\mathbf Y | \mathbf X = \mathbf x]$$`
"Change in CEF *holding other variables constant*"

Small set of **other variables** `\(\to\)` easy (human) understanding

---

## Lessons about sparsity


### Sparsity helps with interpretation

### and solving otherwise impossible problems

*Curse of dimensionality* / NP-hard optimization (best subsets) / unidentifiable statistical estimation / **overfitting vs generalization** (next)

Need special mathematical structure like sparsity to make things tractable

---
class: inverse, middle, center

# Empirical risk

## is a biased estimator

### that understimates risk

Generally, more optimization `\(\to\)` more bias

But! Formulas for the bias in certain cases

---

## Optimism / generalization gap

Observation: training error generally appears lower than test/validation error. Why?

Risk minimization vs *empirical* risk minimization

`$$R(g) = \mathbb E_F[L(\mathbf X,Y,g)]$$`

`$$\hat f =  \arg \min_g \hat R(g) = \arg \min_g \frac{1}{n} \sum_{i=1}^n L(\mathbf x_i, y_i, g)$$`

--

**Fact**: for some `\(\text{df}(\hat f) &gt; 0\)` (depends on problem/fun. class)

`$$\Delta = \mathbb E_{Y|\mathbf x_1, \ldots, \mathbf x_n}[R(\hat f) - \hat R(\hat f)] = \frac{2\sigma^2_\varepsilon}{n} \text{df}(\hat f) &gt; 0$$`



---

### Optimism and degrees of freedom

#### Linear case

If `\(\hat f\)` is linear with `\(p\)` predictors (or `\(p\)` basis function transformations of original predictors) then

$$
\text{df}(\hat f) = p, \text{ so } \Delta = 2 \sigma^2_\varepsilon \frac{p}{n} 
$$

#### Fairly general case

For many ML tasks and fitting procedures

`$$\text{df}(\hat f) = \frac{1}{\sigma^2_\varepsilon} \text{Tr}[ \text{Cov}(\hat f (\mathbf X), \mathbf y) ] = \frac{1}{\sigma^2_\varepsilon} \sum_{i=1}^n \text{Cov}(\hat f (\mathbf x_i), y_i)$$`
---

### Lasso degrees of freedom

The "0-norm" (not really a norm) counts sparsity

`$$\| \beta \|_0 = \sum_{j=1}^p \mathbf 1_{\beta_j \neq 0} = | \{ j : \beta_j \neq 0 \} |$$`
e.g. for OLS with deterministic choice of variables

`$$\text{df}(\hat \beta) = \| \hat \beta \|_0$$`

**Surprisingly**, under fairly weak conditions on `\(\mathbf X\)` (columns in general position), for the lasso solution `\(\hat \beta(\lambda)\)` at any `\(\lambda\)`

`$$\mathbb E[\| \hat \beta(\lambda) \|_0] = \text{df}(\hat \beta(\lambda))$$`
*Solution sparsity is unbiased estimate of df, and same as OLS!*

---

## Choosing/penalizing complexity

**Idea**: if we have a formula to estimate optimism/gen. gap then change the loss function to
$$
\text{minimize } \hat L_\Delta(g) = \hat R(g) + \hat \Delta(g)
$$

e.g. `\(C_p\)`, AIC, BIC, etc

**Problem**: optimization bias, if `\(\hat g\)` is the minimizer then

`$$\mathbb E [ \hat L_\Delta(\hat g) ] &lt; L_\Delta(\hat g)$$`

Again, more optimization (larger model search) `\(\to\)` more bias

**Benefit**: may be more reproducible than random data splitting

---

### Lessons about optimism and generalization

- Empirical risk underestimates actual risk (expected loss on new sample from same distribution)

- Magnitude of the bias is the optimism / generalization gap

- Optimism generally increases with function class complexity

  - e.g. for linear functions, increases linearly in `\(p\)`

- For a fixed function class, optimism decreases linearly in `\(n\)`

- More optimization `\(\to\)` overfitting `\(\to\)` more optimism


---
class: inverse, center, middle


# Estimate test error directly

## using test data

--

### i.e. a new set of data, "unseen" by `\(\hat f\)`

Indep. samples `\(D = \{  (\mathbf x_i, y_i) \}_{i=1}^n\)` and `\(D' = \{ (\mathbf x_i', y_i') \}_{i=1}^{n'}\)`

Estimate `\(\hat f\)` on `\(D\)`, evaluate `\(\hat f\)` on `\(D'\)`

---

### Motives: deploying an algorithm "in production"

Actual practice at some organizations

Fit model at some time

Use model for predictions on new data 

Possibly re-fit/update model periodically

---

### Motives: phil. of science, novelty, and severity

[Philosophy of science](https://plato.stanford.edu/entries/prediction-accommodation/): prediction vs "accommodation"

Prediction: happens in time before observation/measurement

Accommodation: theory built to explain past observation/data

Accurate prediction is better evidence in favor of a scientific theory than mere accommodation

ML: What's better evidence in favor of the model?

--

Popper and Lakatos: **temporal novelty**

Zahar, Gardner, Worrall: **use-novelty** (or problem novelty)

[Mayo](http://bactra.org/reviews/error/): novelty is not necessary. **Severity** is necessary

---

# Choosing model complexity

## Using test/validation data

Indep. samples `\(D = \{  (\mathbf x_i, y_i) \}_{i=1}^n\)` and `\(D' = \{ (\mathbf x_i', y_i') \}_{i=1}^{n'}\)`

- Estimate `\(\hat f_\lambda\)` on `\(D\)` for a "path" or grid of `\(\lambda\)` values

- Evaluate `\(\hat f_\lambda\)` on `\(D'\)` and choose `\(\hat \lambda\)` accordingly (e.g. with minimum loss)

- Refit `\(\hat f_{\hat \lambda}\)` on full data `\(D \cup D'\)`, this is our final model

---

# Cross-validation

**Idea**: swap `\(D\)` and `\(D'\)` in previous process and get two estimates, `\(\hat R(\hat f_\lambda)\)` and `\(\hat R(\hat f_\lambda')\)`

Average these and choose `\(\hat \lambda\)` using the average (e.g. minimizer)

**Idea**: apply the same process with multiple independent "folds" of data

#### `\(K\)`-fold cross-validation

Each subset used once as test set, and `\(K-1\)` times for training

Minimize `\(\hat R_{K\text{-cv}}(\lambda) = \frac{1}{K} \sum_{k=1}^K \hat R_k(\hat f^{(k)}_\lambda)\)`

---

## plot(cv.glmnet) and plot(glmnet)

.pull-left[

```r
cv_fit &lt;- cv.glmnet(X, Y)
plot(cv_fit)
```

&lt;img src="06-1-regularization-phd_files/figure-html/unnamed-chunk-10-1.png" width="504" /&gt;
]
.pull-right[

```r
plot_glmnet(lasso_fit,
    s = cv_fit$lambda.1se)
```

&lt;img src="06-1-regularization-phd_files/figure-html/unnamed-chunk-11-1.png" width="504" /&gt;
]

---

## Lessons about cross-validation

- Think of it as a way to **choose model complexity**

- **Beware** common cross-validation errors! From Duda and Hart quoted in [MLstory](https://mlstory.org/data.html)

&gt; ... the same data were often used for designing and testing the classifier. This mistake is frequently referred to as "testing on the training data." A related but less obvious problem arises *when a classifier undergoes a long series of refinements guided by the results of repeated testing on the same data. This form of "training on the testing data" often escapes attention until new test samples are obtained*.

---

## Lessons about cross-validation

- **Beware** common cross-validation errors! From ESL, also quoted in [MLstory](https://mlstory.org/data.html)

&gt; Ideally, the test set should be kept in a "vault," and be brought out only at the end of the data analysis. *Suppose instead that we use the test-set repeatedly, choosing the model with smallest test-set error. Then the test set error of the final chosen model will underestimate the true test error*, sometimes substantially.

- Cross-validate the entire pipeline, not just one step

- Choosing `\(K\)`: larger `\(\to\)` `\(\hat R_{K\text{-cv}}\)` has lower bias, more variance

---
class: inverse, middle, center

# Inference

## for high-dimensional regression

We have used regularization to avoid overfitting

But this results in bias, e.g. `\(\| \hat \beta \|\)` is smaller

Inference must correct for this somehow

---

## Approaches to inference

- Debiased inference
- Selective inference
- Post-selection inference
- Stability selection

`R` packages for some of these

Topic for future study? 😁

---

## One example


```r
set.seed(1)
n &lt;- 100; p &lt;- 200
X &lt;- matrix(rnorm(n*p), nrow = n)
beta = sample(c(-1, rep(0, 20), 1), p, replace = TRUE)
Y &lt;- X %*% beta + rnorm(n)
```



Cross-validation plot (next slide)


```r
beta_hat &lt;- coef(lasso_fit, s = cv_lasso$lambda.min)[-1]
vars &lt;- which(beta_hat != 0)
vars
```

```
## [1]  24  34  43  90 111 125 156 168
```

**Idea**: since `\(\hat \beta\)` is biased by penalization, how about refitting OLS (unpenalized) using only these variables?


---

&lt;img src="06-1-regularization-phd_files/figure-html/unnamed-chunk-15-1.png" width="504" /&gt;



---


```r
summary(lm(Y ~ X[,vars]-1))
```

```
## 
## Call:
## lm(formula = Y ~ X[, vars] - 1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.5978 -0.2385  0.2125  0.7718  2.6094 
## 
## Coefficients:
##            Estimate Std. Error t value Pr(&gt;|t|)    
## X[, vars]1  0.23733    0.08711   2.725  0.00771 ** 
## X[, vars]2 -0.13354    0.08056  -1.658  0.10079    
## X[, vars]3 -0.16768    0.07682  -2.183  0.03160 *  
## X[, vars]4 -0.30632    0.08171  -3.749  0.00031 ***
## X[, vars]5 -0.13511    0.07967  -1.696  0.09330 .  
## X[, vars]6  0.18805    0.08077   2.328  0.02209 *  
## X[, vars]7 -0.14805    0.08157  -1.815  0.07279 .  
## X[, vars]8 -0.15296    0.08071  -1.895  0.06121 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.8091 on 92 degrees of freedom
## Multiple R-squared:  0.3747,	Adjusted R-squared:  0.3203 
## F-statistic: 6.891 on 8 and 92 DF,  p-value: 4.501e-07
```

---

# Looks good, time to publish!

- Sparse, interpretable model
- Some significant predictors
- Decent `\(R^2\)` value showing predictive accuracy

Pretty good... hey wait, what was this line in the code...

--


```r
*Y &lt;- rnorm(n)
lasso_fit &lt;- glmnet(X, Y)
cv_lasso &lt;- cv.glmnet(X, Y)
```

Model was *actually* fit on pure noise

😱, 😱, 😱, 😱, 😱, 😱, 😱, 😱, 😱, 😱, 😱, 😱, 😱, 😱, 😱, 😱, 😱, 😱, 😱

**Idea**: compute inferences (`summary`) on new validation data

---

### Lessons about high-dimensional regression

- Can fit to noise, even with cross-validation

- Theoretical results

Lasso "performs well" (prediction error, estimation error, sparse support recovery) under various sets of sufficient conditions, derived/proven using KKT conditions and probability bounds (see SLS Chapter 11)

Roughly:

- `\(\mathbf X\)` has to be well-conditioned in some sense
- True `\(\beta\)` has to be sparse enough
- Solution still generally includes some [false positives](https://projecteuclid.org/journals/annals-of-statistics/volume-45/issue-5/False-discoveries-occur-early-on-the-Lasso-path/10.1214/16-AOS1521.full)

---

# References

Optimism / generalization gap (ESL 7.4-6)

Covariance penalty and degrees of freedom (CASI 12.3)

Cross-validation (ESL 7.10)

SLS 2 for lasso, cross-validation, degrees of freedom

CASI 16 for lasso

CASI 20 / SLS 6 for inference after model selection

SLS 5.2 for KKT conditions

SLS 11 for theoretical resuls about lasso
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"highlightSpans": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
(function(time) {
  var d2 = function(number) {
    return ('0' + number).slice(-2); // left-pad 0 to minutes/seconds
  },

  time_format = function(total) {
    var secs = Math.abs(total) / 1000;
    var h = Math.floor(secs / 3600);
    var m = Math.floor(secs % 3600 / 60);
    var s = Math.round(secs % 60);
    var res = d2(m) + ':' + d2(s);
    if (h > 0) res = h + ':' + res;
    return res;  // [hh:]mm:ss
  },

  slide_number_div = function(i) {
    return document.getElementsByClassName('remark-slide-number').item(i);
  },

  current_page_number = function(i) {
    return slide_number_div(i).firstChild.textContent;  // text "i / N"
  };

  var timer = document.createElement('span'); timer.id = 'slide-time-left';
  var time_left = time, k = slideshow.getCurrentSlideIndex(),
      last_page_number = current_page_number(k);

  setInterval(function() {
    time_left = time_left - 1000;
    timer.innerHTML = ' ' + time_format(time_left);
    if (time_left < 0) timer.style.color = 'red';
  }, 1000);

  slide_number_div(k).appendChild(timer);

  slideshow.on('showSlide', function(slide) {
    var i = slide.getSlideIndex(), n = current_page_number(i);
    // reset timer when a new slide is shown and the page number is changed
    if (last_page_number !== n) {
      time_left = time; last_page_number = n;
      timer.innerHTML = ' ' + time_format(time); timer.style.color = null;
    }
    slide_number_div(i).appendChild(timer);
  });
})(59000);
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
