---
title: "Kernel SVM and non-linear regression"
author: "Joshua Loftus"
date: "2/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)  
library(tidymodels) 
library(modelr)     
library(kernlab)    
library(gapminder)  
theme_set(theme_minimal(base_size = 22))
```

## 2d non-linear classification data

```{r}
set.seed(1)
# sample size
n <- 500
# function with zero set that
# defines the perfect (Bayes) decision boundary
true_boundary_function <- function(x1, x2) {
  # experiment with changing this
  #(x1^2 + x2^2 - 1)^3 - x1^2*x2^3
  abs(x1*x2) - x1
}
train <- data.frame(
  # change the distribution
  # or scale as needed
  x1 = 1.5*(1 - 2*runif(n)),
  x2 = 1.5*(1 - 2*runif(n))
) %>% 
  mutate(
    # labels if classes were perfectly separated
    separable = true_boundary_function(x1,x2) > 0,
    # labels if classes are "noisy"
    y = factor(rbinom(n, 1, 9/10 - (8/10) * as.numeric(separable)))
  )
```

### Plot the data

```{r}
train_plot <-
  ggplot(train, aes(x1, x2)) +
  geom_point(aes(shape = y, color = y),
             alpha = .5, show.legend = FALSE) +
  xlab("") + ylab("")
  
train_plot
```

### Plot the Bayes decision boundary

```{r}
decision_surface <- 
  data_grid(train,
          x1 = seq_range(x1, 300, expand = .05),
          x2 = seq_range(x2, 300, expand = .05)) %>%
  mutate(z = true_boundary_function(x1, x2))

boundary_plot <-
  train_plot +
  geom_contour(
    data = decision_surface,
    aes(x1, x2, z=z),
    bins = 2,
    size = 1,
    color = "black",
    alpha = .5)

boundary_plot
```

### How well does linear classification do?

Also try changing the formula to fit a logistic regression model with non-linear transformations of the predictors

```{r}
# Fit model
logit_model <-  
  glm(y ~ x1 + x2 + poly(x1,2) * poly(x2,2), family = "binomial", data = train)
# try formula + poly(x1,2) * poly(x2,2)

# Generate decision boundary
logit_surface <- logit_model %>%
   augment(type.predict = "response",
              newdata = decision_surface)
```

Plot decision boundary of logistic model

```{r}
boundary_plot +
  geom_contour(
    data = logit_surface,
    aes(x1, x2, z = .fitted),
    size = 1,
    color = "green",
    bins = 2)
```

## Kernel SVM

Use the `ksvm` function from the `kernlab` package to fit a non-linear classification models and plot them

Try different kinds of kernels

Experiment with cost/complexity parameters

Optional: change the decision boundary function, noise level, sample size

```{r}
k_fit <- ksvm(y ~ x2 + x1,
                  kernel = "rbfdot",
                  kpar = list(sigma = 2),
                  C = 20,
                  #nu = 0.05,
                  data = train)
k_fit
```

```{r}
plot(k_fit)
```

Use the `alpha()` function to find the support vector points and plot them

```{r}
kernlab::alpha(k_fit)
```


## 1-d smooth regression example

Generate a one-dimensional example with a non-linear relationship

Experiment with changing `n` and `sd`

```{r}
# change this function
f <- function(x) sin(4*pi*x)
n <- 200
train1d <- 
  data.frame(
    x = rbeta(n, 1, 3)
    ) %>%
  # change the noise level sd
  mutate(y = f(x) + rnorm(n, sd = .1))

# plot data and loess curve
ggplot(train1d, aes(x, y)) + 
  geom_point() +
  geom_smooth()
```

Linear regression with a polynomial transform of x

```{r}
model_lm <- lm(y ~ poly(x, 5), data = train1d)

train1d_grid <- 
  data_grid(train1d,
          x = seq_range(c(x, .78), 1000, expand = .05))

augment(model_lm,
        newdata = train1d_grid) %>%
  ggplot(aes(x, y)) +
  geom_point(data = train1d) +
  geom_line(aes(y = .fitted)) +
  ylim(c(-2,2))
```

## Kernel regression

Use the `ksvm` function and `kernelMatrix` functions to fit non-linear kernel regression models and plot the predictions on `train1d_grid`


```{r}
k_1fit <- ksvm(y ~ x,
     kernel = rbfdot(sigma = 1),
     data = train1d)
k_1fit
```

```{r}
svm_predictions <- train1d_grid %>%
      mutate(.fitted = predict(k_1fit, newdata = train1d_grid))
train1d %>%
  ggplot(aes(x, y)) +
  geom_point(data = train1d) +
  geom_line(
    data = svm_predictions,
    aes(y = .fitted)) #+ylim(c(-2,2))
```

Compare the predictions using a `polydot` kernel with those from a (global) polynomial regression

Optional: Compare predictions with `rbfdot` to those from our implementation of local regression

## gapminder

Repeat the above using the `gapminder` data

```{r}

```

Perform and out-of-distribution test by computing the accuracy or plotting the predictions for a different year of data

```{r}

```

