---
title: "Machine learning"
subtitle: "Classification and logistic regression"
author: "Joshua Loftus"
#institute: "LSE"
#date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "xaringan-themer.css"]
    seal: true    
    lib_dir: libs
    nature:
      titleSlideClass: ["top", "left"]
      countdown: 59000
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
---

class: inverse

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE)
options(knitr.table.format = "html")
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#2d708e",
  secondary_color = "#230433",
  link_color = "#55c667",
  text_bold_color = '#f68f46',
  title_slide_background_color = "#ffffff", #"#042333",
  title_slide_background_image = "../../../files/theme/LSE/centre_building_day.jpg",
#    "https://upload.wikimedia.org/wikipedia/commons/1/1a/Workhouse_Nantwich.jpg",
  title_slide_background_size = "cover",
  ) #or contain
```

```{r xaringanextra, include=FALSE, warning=FALSE}
library(xaringanExtra)
xaringanExtra::use_animate_all("slide_left")
```

```{r tidyverse, include=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
theme_set(theme_minimal(base_size = 22))
set.seed(1)
library(broom)
library(modelr)
```

<style type="text/css">
.remark-slide-content {
    font-size: 1.2rem;
    padding: 1em 4em 1em 4em;
}
</style>

## Classification

- **Supervised learning** with categorical/qualitative outcomes

(in contrast to regression, with numeric outcomes)

- Often called "labels", $K$ = number of unique classes

- Binary: positive/negative or 0/1 or yes/no or success/fail etc

*Label names not mathematically important* - e.g. use $1, ..., K$

--

- Limitations: labels already defined (not learned from data-- that  would be unsupervised learning), $K$ is fixed

- Plots: often use color/point shape for categorical variables

---

## Interpretable classification

### Logistic regression 

$$\mathbb E(Y| \mathbf X = \mathbf x) =  g^{-1}(\mathbf x^T \beta)$$
$$g(p) = \log{\left(\frac{p}{1-p}\right)}$$
--

### Generalized linear models (GLMs)

- [Various](https://en.wikipedia.org/wiki/Generalized_linear_model#General_linear_models) "link" functions $g$
- Linear regression is a special case with $g = \text{id}$
- Logistic in `R`: `glm(..., family = binomial())`
- Others: Poisson, multinomial, ..., see `?family` in `R`

---

```{r logit-data, echo = FALSE}
set.seed(42)
n <- 80
Y <- rbinom(n, 1, .5)
train <- data.frame(
  y = factor(Y),
  x1 = rnorm(n) + 1.5*Y,
  x2 = rnorm(n) + 1.2*Y
)
```

### One predictor, "S curve"

```{r logit-1dm, echo = FALSE, fig.align='center', out.width="540px"}
eg1 <- ggplot(train, aes(x1, y)) +
  geom_point(aes(color = y), show.legend = FALSE, alpha = .4) +
  geom_line(
    data = augment(
      glm(y ~ x1, family = "binomial", data  = train),
      newdata = data_grid(data = train,
                          x1 = seq_range(x1, 100, expand = .1)),
      type.predict = "response"
    ),
    aes(x1, .fitted + 1),
    size = 1
  ) + scale_color_viridis_d(option = "D", end = .8) +
  annotate("text", x = 0.2, y = 1.5, label = "g^{-1}", size = 6, parse = TRUE) 
eg1
```

---

### Classifications/decisions: threshold probability

```{r logit-1d-class, echo = FALSE, fig.align='center', out.width="540px"}
eg1 +
  annotate("segment", x = .9, xend = .9, y = .8, yend  = 2.2, linetype = "dashed", size = 1, color = "blue") +
  geom_point(data =  train,
             aes(color =  y), alpha =  .4,
             show.legend = FALSE) +
  annotate("text", x = -.25, y = 2.2, color = "red",
           label = "false negatives", size = 6) +
  annotate("text", x = 2, y = 2.2,
           label = "true positives", size = 6) +
  annotate("text", x = -.25, y = .8,
           label = "true negatives", size = 6) +
  annotate("text", x = 2, y = .8, color = "red",
           label = "false positives", size = 6) 
```

---

### Without giving $y$ a spatial dimension

```{r logit-0d-class, echo = FALSE, fig.align='center', out.width="540px"}
ggplot(train, aes(x1, y = 0)) +
  geom_point(aes(color = y), show.legend = FALSE, alpha =  .4) +
  scale_color_viridis_d(option = "D", end = .8) +
  ylim(c(-.2,.2)) +
  annotate("segment", x = .9, xend = .9, y = -.1, yend  = .1, linetype = "dashed", size = 1, color = "blue")
```


---

### Two predictors, binary outcome

```{r logit-data-plot, echo = FALSE, fig.align='center', out.width="540px"}
train_plot <-
  ggplot(train, aes(x1, x2)) +
  geom_point(aes(shape = y, fill = y),
             color = "black", size = 2, stroke = 1,
             show.legend = FALSE, alpha =  .4) +
  scale_shape_manual(values=c(21, 22)) + 
  scale_fill_viridis_d(direction = -1, end = .8)
train_plot
```


---

### Contours of GLM-predicted class probabilities

```{r logit-contour, echo = FALSE, fig.align='center', out.width="540px"}
logit_model <-  glm(y ~ x1 + x2, family = "binomial", data = train)
logit_surface <- logit_model %>%
   augment(type.predict = "response",
              newdata = data_grid(data = train,
                                  x1 = seq_range(x1, 100, expand = .1),
                                  x2 = seq_range(x2, 100, expand = .1)))
ggplot(
  data = logit_surface,
  aes(x1,x2)) +
  geom_contour_filled(
    aes(z = .fitted),
    show.legend = FALSE,
    alpha = .3,
    ) +
  geom_abline(slope = -coef(logit_model)[2]/coef(logit_model)[3],
              intercept = 1.45, linetype = "dashed", size = 1, color = "blue") +
  geom_point(
    data = train,
             aes(shape = y, color = y),
             size = 2, stroke = 1, alpha = .9,
             show.legend = FALSE) +
  scale_shape_manual(values=c(21, 22)) + 
  scale_color_viridis_d(option = "viridis", direction = -1, end = .8) + 
  scale_fill_viridis_d(option = "viridis", end = .8) +
  geom_point(
    data = augment(logit_model,
                   type.predict = "response") %>%
      mutate(yhat = as.numeric(.fitted > .5),
             class = as.numeric(y)-1 == yhat) %>%
      filter(class == FALSE),
    aes(shape = y, color = y),
             stroke = 1.5, alpha = .9,
             show.legend = FALSE,
    fill = "red", size = 3)
```

---
class: middle, center

**Classification boundaries** with

## $p = 3$ predictors

### Boundary = plane

## $p > 3$ predictors

### Boundary = hyperplane

(In practice, "high-dimensional" = can't easily plot it)

---

### Interpretation: coefficients

```{r}
model_fit <- glm(y ~ x1 + x2, family = "binomial", data = train)
broom::tidy(model_fit)
```

Coefficient scale: log-odds? Exponentiate $\to$ odds

```{r}
broom::tidy(model_fit, exponentiate = TRUE)
```

---

### Interpretation: inference and diagnostics

- MLEs $\to$ asymptotic normality for intervals/tests

`summary()`, `coef()`, `confint()`, `anova()`, etc in `R`

- "Deviance" instead of RSS

```{r}
broom::glance(model_fit)
```


- Because $y$ is 0 or 1, residual plots will show patterns, not as easy to interpret geometrically


---

###  Challenges

#### Separable case (guaranteed if $p > n$)

If classes can be perfectly separated, the MLE is undefined, fitting algorithm diverges as $\hat \beta$ coordinates $\to \pm \infty$

Awkwardly, classification is *too easy*(!?) for this probabilistic approach

--

#### Curse of dimensionality

Biased MLE and wrong variance/asymp. dist. if $n/p \to \text{const}$, even if $> 1$

.footnote[See [Sur and CandÃ¨s, (2019)](https://www.pnas.org/content/116/29/14516)]

---
class: inverse

### Classification summary

- Numeric prediction $\to$ classification

$$\hat y = \mathbb I(\hat p > c) = \begin{cases} 0 & \text{ if } \hat y \leq c \\  1 & \text{ if } \hat y > c \end{cases}$$

Log-odds function is monotonic, so (hyperplanes)

$$\hat p > c \leftrightarrow x^T \beta > c'$$

- More classes: transform to binary, predict using largest $\hat p_k$

- Non-linear boundaries: transformation of predictors, or use methods other than GLMs (we'll learn more soon)

- Some classification methods output categorical classes, not probabilities (or other numeric scores)

---

### Fitting logistic regression

How do we estimate $\beta$? **Maximum likelihood**:

$$
\text{maximize } L(\beta ; \mathbf y | \mathbf X) = \prod_{i=1}^n L(\beta ; y_i | \mathbf x_i)
$$
(assuming the data is i.i.d.)

Next slide: a bit of mathematics

---

### MLE

$$
L(\beta ; \mathbf y | \mathbf x) = \prod_{i=1}^n \left( \frac{1}{1+e^{-x_i \beta}} \right)^{y_i} \left(1- \frac{1}{1+e^{-x_i \beta}} \right)^{1-y_i}
$$

--

$$
\ell(\beta ; \mathbf y | \mathbf x) = \sum_{i=1}^n y_i \log \left( \frac{1}{1+e^{-x_i \beta}} \right) + (1-y_i) \log \left(1- \frac{1}{1+e^{- x_i \beta}} \right)
$$

--

$$
\frac{\partial}{\partial \beta} \ell(\beta ; \mathbf y | \mathbf x) = \sum_{i=1}^n y_i  \left( \frac{x_i e^{-x_i \beta}}{1+e^{-x_i \beta}} \right) + (1-y_i) \left(\frac{-x_i}{1+e^{- x_i \beta}} \right)
$$

$$= \sum_{i=1}^n x_i \left[ y_i - \left(\frac{1}{1+e^{- x_i \beta}} \right) \right] = \color{skyblue}{\sum_{i=1}^n x_i [y_i - \hat p_i(\beta)]}$$

Set this equal to 0 and solve for $\beta$ using Newton-Raphson

---

### Newton-Raphson

- Find the roots of a function
- Iteratively approximating the function by its tangent
- Root of the tangent line is used as starting point for next approximation
- See the [animation](https://en.wikipedia.org/wiki/Newton%27s_method#/media/File:NewtonIteration_Ani.gif) on [Wikipedia](https://en.wikipedia.org/wiki/Newton%27s_method)

**Exercise**: using result from previous slide, compute the second derivative of $\ell$ and derive the expressions needed to apply Newton-Raphson

---

### Logistic regression fitting: multivariate case

Newton-IRLS (equivalent) steps:

$$
\begin{eqnarray}
\hat{\mathbf p}_t & = & g^{-1}(\mathbf X \hat \beta_t)
& \ \text{ update probs.} \\
\mathbf W_t & = & \text{diag}[\hat{\mathbf p}_t (1 - \hat{\mathbf p}_t)]  
& \ \text{ update weights} \\
\hat{\mathbf{y}}_t & = & g(\hat{\mathbf p}_t) + \mathbf W_t^{-1}(\mathbf y - \hat{\mathbf p}_t)
& \ \text{ update response}
\end{eqnarray}
$$

and then update parameter estimate (LS sub-problem)

$$\hat{\beta}_{t+1} = \arg \min_{\beta} (\hat{\mathbf{y}}_t - \mathbf X \beta)^T \mathbf W_t (\hat{\mathbf{y}}_t - \mathbf X \beta)$$ 

--

**Note**: larger weights on observations with $\hat p$ closer to 1/2, i.e. the most difficult to classify (***look for variations of this theme***)

.footnote[See Section 4.4.1 of [ESL](https://web.stanford.edu/~hastie/ElemStatLearn/)]

---

### Optimization algorithms

Downside of Newton-Raphson: requires second derivatives, including *inverting the $p \times p$ Hessian matrix* when optimizing over $p > 1$ parameters

If $p$ is large, **second-order** optimization methods like Newton's are very costly

--

First order methods only require computing the $p \times 1$ gradient vector

Recall that the gradient is a vector in the *direction of steepest increase* in the parameter space

---

### Gradient (steepest) descent

i.e. skiing as fast as possible. Notation, let

$$L(\beta) = L(\mathbf X, \mathbf y, g_\beta) \color{skyblue}{\text{ (loss function)}}$$

1. Start at an initial point $\beta^{(0)}$

2. For step $n = 1, \ldots$
  - Compute $\mathbf d_n = \nabla L(\beta^{(n-1)}) \color{skyblue}{\text{ (gradient)}}$
  
  - Update $\beta^{(n)} = \beta^{(n-1)} - \gamma_n \mathbf d_n$
  
3. Until some **convergence criteria** is satisfied

--

Where the **step size** $\gamma_n > 0$ is made small enough to not "overshoot" and increase the loss, i.e. the loss only decreases


---
class: inverse

## Optimization more generally

- Components: objective functions, algorithms, local/global optima, approximate solutions

- Computational cost: speed, storage (time and space)

#### Closed form / analytic solutions

e.g. OLS formula for $\hat \beta$ (remember?)

#### Iterative algorithms (e.g. Newton-Raphon)

- Rates of convergence

- Might have guarantees, e.g. if objective is **convex**

---
class: inverse

Machine learning = optimization algorithms applied to data

Understanding optimization is very important!

- Intuition (challenge: dimensionality)

- Mathematical guarantees (challenge: relevance)

- Empirical evaluation (challenge: overfitting...)