---
title: "Machine learning"
subtitle: "Classification, part 1"
author: "Joshua Loftus"
#institute: "LSE"
#date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "xaringan-themer.css", "../../../theme.css"]
#    seal: false    
    lib_dir: libs
    nature:
      titleSlideClass: ["bottom", "left"]
      countdown: 59000
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
---

class: inverse

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE)
options(knitr.table.format = "html")
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#2d708e",
  secondary_color = "#230433",
  link_color = "#55c667",
  text_bold_color = '#f68f46',
  title_slide_background_color = "#ffffff", #"#042333",
  title_slide_background_image = "../../../files/theme/LSE/centre_building_day.jpg",
#    "https://upload.wikimedia.org/wikipedia/commons/1/1a/Workhouse_Nantwich.jpg",
  title_slide_background_size = "cover",
  ) #or contain
```

```{r xaringanextra, include=FALSE, warning=FALSE}
library(xaringanExtra)
xaringanExtra::use_animate_all("slide_left")
xaringanExtra::use_tile_view()
xaringanExtra::use_panelset()
xaringanExtra::style_panelset_tabs(font_family = "inherit")
```

```{r tidyverse, include=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
theme_set(theme_minimal(base_size = 22))
set.seed(1)
library(broom)
library(modelr)
```

<style type="text/css">
.remark-slide-content {
    font-size: 1.2rem;
    padding: 1em 4em 1em 4em;
}
</style>

## Classification

- **Supervised learning** with categorical/qualitative outcomes

(in contrast to regression, with numeric outcomes)

--

- Often called "labels", $K$ = number of unique classes

*Label names not mathematically important* - e.g. use $1, ..., K$
--

- Binary: positive/negative or 0/1 or yes/no or success/fail etc

- Binary thinking easier $\rightarrow$ bin / discretise other outcomes and do binary classification instead of regression or $K > 2$ classification (warning: information loss)

- Plots with 2 predictors, use color/point shape for outcomes

---

## Interpretable classification

### Logistic regression 

$$\mathbb E(Y| \mathbf X = \mathbf x) =  g^{-1}(\mathbf x^T \beta)$$
for
$$g(p) = \log{\left(\frac{p}{1-p}\right)}$$

--

- Other GLMs have different "link" functions $g$
- (Linear regression is a special case with $g = \text{id}$)
- Multi-class / multinomial / "softmax" regression
- Estimation/optimization: maximum-likelihood via Newton-Raphson / IRLS

---

```{r logit-data, echo = FALSE}
set.seed(42)
n <- 50
Y <- rbinom(n, 1, .5)
train <- data.frame(
  y = factor(Y),
  x1 = rnorm(n) + 1.5*Y,
  x2 = rnorm(n) + 1.2*Y
)
```

### One predictor, "S curve"

```{r logit-1dm, echo = FALSE, fig.align='center', out.width="540px"}
eg1 <- ggplot(train, aes(x1, y)) +
  geom_point(aes(color = y), show.legend = FALSE, alpha = .4) +
  geom_line(
    data = augment(
      glm(y ~ x1, family = "binomial", data  = train),
      newdata = data_grid(data = train,
                          x1 = seq_range(x1, 100, expand = .1)),
      type.predict = "response"
    ),
    aes(x1, .fitted + 1),
    size = 1
  ) + scale_color_viridis_d(option = "D", end = .8) +
  annotate("text", x = 0.2, y = 1.5, label = "g^{-1}", size = 6, parse = TRUE) 
eg1
```

---

### Classifications/decisions: threshold probability

```{r logit-1d-class, echo = FALSE, fig.align='center', out.width="540px"}
eg1 +
  annotate("segment", x = .9, xend = .9, y = .8, yend  = 2.2, linetype = "dashed", size = 1, color = "blue") +
  geom_point(data =  train,
             aes(color =  y), alpha =  .4,
             show.legend = FALSE) +
  annotate("text", x = -.25, y = 2.2, color = "red",
           label = "false negatives", size = 6) +
  annotate("text", x = 2, y = 2.2,
           label = "true positives", size = 6) +
  annotate("text", x = -.25, y = .8,
           label = "true negatives", size = 6) +
  annotate("text", x = 2, y = .8, color = "red",
           label = "false positives", size = 6) 
```

---

### Without giving $y$ a spatial dimension

```{r logit-0d-class, echo = FALSE, fig.align='center', out.width="540px"}
ggplot(train, aes(x1, y = 0)) +
  geom_point(aes(color = y), show.legend = FALSE, alpha =  .4) +
  scale_color_viridis_d(option = "D", end = .8) +
  ylim(c(-.2,.2)) +
  annotate("segment", x = .9, xend = .9, y = -.1, yend  = .1, linetype = "dashed", size = 1, color = "blue")
```


---

### Two predictors, binary outcome

```{r logit-data-plot, echo = FALSE, fig.align='center', out.width="540px"}
train_plot <-
  ggplot(train, aes(x1, x2)) +
  geom_point(aes(shape = y, fill = y),
             color = "black", size = 2, stroke = 1,
             show.legend = FALSE, alpha =  .4) +
  scale_shape_manual(values=c(21, 22)) + 
  scale_fill_viridis_d(direction = -1, end = .8)
train_plot
```


---

### Contours of GLM-predicted class probabilities

```{r logit-contour, echo = FALSE, fig.align='center', out.width="540px"}
logit_model <-  glm(y ~ x1 + x2, family = "binomial", data = train)
logit_surface <- logit_model %>%
   augment(type.predict = "response",
              newdata = data_grid(data = train,
                                  x1 = seq_range(x1, 100, expand = .1),
                                  x2 = seq_range(x2, 100, expand = .1)))
ggplot(
  data = logit_surface,
  aes(x1,x2)) +
  geom_contour_filled(
    aes(z = .fitted),
    show.legend = FALSE,
    alpha = .3,
    ) +
  geom_abline(slope = -coef(logit_model)[2]/coef(logit_model)[3],
              intercept = 3.05, linetype = "dashed", size = 1, color = "blue") +
  geom_point(
    data = train,
             aes(shape = y, color = y),
             size = 2, stroke = 1, alpha = .9,
             show.legend = FALSE) +
  scale_shape_manual(values=c(21, 22)) + 
  scale_color_viridis_d(option = "viridis", direction = -1, end = .8) + 
  scale_fill_viridis_d(option = "viridis", end = .8) +
  geom_point(
    data = augment(logit_model,
                   type.predict = "response") %>%
      mutate(yhat = as.numeric(.fitted > .56),
             class = as.numeric(y)-1 == yhat) %>%
      filter(class == FALSE),
    aes(shape = y, color = y),
             stroke = 1.5, alpha = .9,
             show.legend = FALSE,
    fill = "red", size = 3)
```

---
class: middle, center

**Classification boundaries** with

## $p = 3$ predictors

### Boundary = plane

## $p > 3$ predictors

### Boundary = hyperplane

(In practice, "high-dimensional" = can't easily plot it)

---

### Fitting/estimation

How do we estimate $\beta$? **Maximum likelihood**:

$$
\text{maximize } L(\beta ; \mathbf y | \mathbf X) = \prod_{i=1}^n L(\beta ; y_i | \mathbf x_i)
$$
(assuming the data is i.i.d.)

--

It's good to have some understanding of what's involved (it's not magic)

Next slide: consider a one-parameter case, one predictor and no intercept, so the calculus simplifies

---

### Logistic regression fitting: MLE `r emo::ji("scream")` jk `r emo::ji("sunglasses")` 

$$
L(\beta ; \mathbf y | \mathbf x) = \prod_{i=1}^n \left( \frac{1}{1+e^{-x_i \beta}} \right)^{y_i} \left(1- \frac{1}{1+e^{-x_i \beta}} \right)^{1-y_i}
$$

--

$$
\ell(\beta ; \mathbf y | \mathbf x) = \sum_{i=1}^n y_i \log \left( \frac{1}{1+e^{-x_i \beta}} \right) + (1-y_i) \log \left(1- \frac{1}{1+e^{- x_i \beta}} \right)
$$

--

$$
\frac{\partial}{\partial \beta} \ell(\beta ; \mathbf y | \mathbf x) = \sum_{i=1}^n y_i  \left( \frac{x_i e^{-x_i \beta}}{1+e^{-x_i \beta}} \right) + (1-y_i) \left(\frac{-x_i}{1+e^{- x_i \beta}} \right)
$$

$$= \sum_{i=1}^n x_i \left[ y_i - \left(\frac{1}{1+e^{- x_i \beta}} \right) \right] = \color{skyblue}{\sum_{i=1}^n x_i [y_i - \hat p_i(\beta)]}$$

Set this equal to 0 and solve for $\beta$ using Newton-Raphson

---

### Newton-Raphson

- Find the roots of a function
- Iteratively approximating the function by its tangent
- Root of the tangent line is used as starting point for next approximation
- See the animation on [Wikipedia](https://en.wikipedia.org/wiki/Newton%27s_method)

--

**Exercise**: using result from previous slide, compute the second derivative of $\ell$ and derive the expressions needed to apply Newton-Raphson

---

### Logistic regression fitting: multivariate case

Newton-IRLS (equivalent) steps:

$$
\begin{eqnarray}
\hat{\mathbf p}_t & = & g^{-1}(\mathbf X \hat \beta_t)
& \ \text{ update probs.} \\
\mathbf W_t & = & \text{diag}[\hat{\mathbf p}_t (1 - \hat{\mathbf p}_t)]  
& \ \text{ update weights} \\
\hat{\mathbf{y}}_t & = & g(\hat{\mathbf p}_t) + \mathbf W_t^{-1}(\mathbf y - \hat{\mathbf p}_t)
& \ \text{ update response}
\end{eqnarray}
$$

and then update parameter estimate:

$$\hat{\beta}_{t+1} = \arg \min_{\beta} (\hat{\mathbf{y}}_t - \mathbf X \beta)^T \mathbf W_t (\hat{\mathbf{y}}_t - \mathbf X \beta)$$ 

--

**Note**: larger weights on observations with $\hat p$ closer to 1/2, i.e. the most difficult to classify (***look for variations of this theme***)

.footnote[See Section 4.4.1 of [ESL](https://web.stanford.edu/~hastie/ElemStatLearn/)]

---

### Inference

- MLEs $\to$ asymptotic normality for intervals/tests

`summary()`, `coef()`, `confint()`, `anova()`, etc in `R`

- "Deviance" instead of RSS

- Because $y$ is 0 or 1, residual plots will show patterns, not as easy to interpret geometrically

Reference: [CASI](https://web.stanford.edu/~hastie/CASI/) Chapter 4 for MLE theory, Chapter 8 for logistic regression and GLMs

---

###  Challenges

#### Separable case (guaranteed if $p > n$)

If classes can be perfectly separated, the MLE is undefined, fitting algorithm diverges as $\hat \beta$ coordinates $\to \pm \infty$

Awkwardly, classification is *too easy*(!?) for this probabilistic approach

--

#### Curse of dimensionality

Biased MLE and wrong variance/asymp. dist. if $n/p \to \text{const}$, even if $> 1$

.footnote[See [Sur and Candès, (2019)](https://www.pnas.org/content/116/29/14516)]

---

### Recap: numeric outcome $\to$ categorical

**Warning**: *arbitrary* binning may be unwise. "*Carve nature at its joints*"

.pull-left[
```{r bimodal, echo = FALSE}
set.seed(1)
Y <- c(rnorm(100), rnorm(70, mean = 4))
X <- c(rnorm(100), rnorm(70, mean = 2))
true_class <- factor(c(rep("A", 100), rep("B", 70)))
cutoff <- Y > 2
data.frame(x = X, y = Y, cutoff = cutoff, class = true_class) %>%
ggplot(aes(X, Y)) + 
  geom_point(aes(shape = class, color = class), show.legend = FALSE) +
  geom_hline(yintercept = 2) +
  geom_rug() +
  scale_color_viridis_d(option = "D", end = .8)
```
]
.pull-right[
```{r unimodal, echo = FALSE}
set.seed(1)
Y <- c(rnorm(100), rnorm(70, mean = 2))
X <- c(rnorm(100), rnorm(70, mean = 2))
true_class <- factor(c(rep("A", 100), rep("B", 70)))
cutoff <- Y > 1
data.frame(x = X, y = Y, cutoff = cutoff, class = true_class) %>%
ggplot(aes(X, Y)) + 
  geom_point(aes(shape = class, color = class), show.legend = FALSE) +
  geom_hline(yintercept = 1) +
  geom_rug() +
  scale_color_viridis_d(option = "D", end = .8)
```
]

---
class: inverse

### Recap

- Numeric prediction $\to$ classification

$$\hat y = \mathbb I(\hat p > c) = \begin{cases} 0 & \text{ if } \hat y \leq c \\  1 & \text{ if } \hat y > c \end{cases}$$

--

- Logistic regression

Log-odds of $\hat p =$ linear function of $x$, so $\hat p > c \leftrightarrow x^T \beta > c'$

Linear classification boundary (hyperplanes)

--

- Optimization problems

Iterative algorithms (e.g. Newton-Raphon)

Adapting to "most interesting" (difficult to classify) data

