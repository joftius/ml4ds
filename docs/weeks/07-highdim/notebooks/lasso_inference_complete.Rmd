---
title: "Lasso inference experiments"
author: "ST310"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) 
library(broom)     
library(glmnet)    
library(selectiveInference)
theme_set(theme_minimal(base_size = 20))
```

## Inference for the lasso

#### Generate data for a high-dimensional regression

```{r}
n <- 100
p <- 200
X <- matrix(rnorm(n*p), nrow = n)
Y <- rnorm(n)
```

#### Fit the lasso solution path and choose a sparsity level automatically

Note: this is an alternative method to cross-validation

```{r}
# Compute lasso path
# while storing information required for
# conditional (truncated) inference
lasso_path <- lar(X, Y)
# Use AIC/BIC to choose model size
# and compute adjusted p-values
# in higher dimensions change mult
# to log(n) for BIC, log(p) for RIC
lasso_inference <- larInf(lasso_path, mult = 2, type = "aic", ntimes = 1)
```

Selected predictors

```{r}
active_set <- lasso_inference$vars
active_set
```

#### Fit an OLS model using only selected variables, compute p-values the classical way

(Use `summary()` or `tidy()` from the `broom` package)

```{r}
OLS_fit <- lm(Y ~ X[, active_set] - 1)
tidy(OLS_fit)
```

#### Adjusted inference

These p-values (from the `larInf` function in the `selectiveInference` package above) are calculated from a conditional distribution, conditional on the model selection

```{r}
lasso_inference
```

#### How do the two sets of p-values compare? Which are "better", and why?

Classical inference is biased toward significance. Even though the true coefficients are all zero we get many significant p-values. This is due to **model selection bias**. (Classical inference methods are based on an assumption of the model being chosen in advance, they become invalid if the model is chosen using the same data)

#### Compare to p-values calculated on a new set of test data

```{r}
X_test <- matrix(rnorm(n*p), nrow = n)
Y_test <- rnorm(n)
lm(Y_test ~ X_test[, active_set] - 1) %>% tidy()
```

Inference calculated using test data is does not suffer from the model selection bias because this data wasn't used to select the model

#### Rerun the code several times and observe the variability

The model changes each time, and so do the p-values, but the qualitative conclusion remains the same:

- Classical inference methods are biased if they are computed on the same data that was used to select the model
- Inference on test data can avoid this problem
- Specialized inference methods can correct the bias without needing more test data (advanced topic)

#### Change `Y` to be generated from a coefficient vector where only the first 5 entries are nonzero and then run the code again a few more times

```{r}
X <- matrix(rnorm(n*p), nrow = n)
beta <- c(rep(1, 5), rep(0, p - 5))
Y <- X %*% beta + rnorm(n)
```

```{r}
X_test <- matrix(rnorm(n*p), nrow = n)
Y_test <- X_test %*% beta + rnorm(n)
lm(Y_test ~ X_test[, active_set] - 1) %>% tidy()
```





