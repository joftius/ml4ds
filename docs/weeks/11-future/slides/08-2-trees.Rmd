---
title: "Machine learning"
subtitle: "Tree-based methods"
author: "Joshua Loftus"
#institute: "LSE"
#date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "xaringan-themer.css", "../../../theme.css"]
#    seal: false    
    lib_dir: libs
    nature:
      titleSlideClass: ["bottom", "left"]
      countdown: 59000
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
---

class: inverse

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE)
options(knitr.table.format = "html")
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#2d708e",
  secondary_color = "#230433",
  link_color = "#55c667",
  text_bold_color = '#f68f46',
  title_slide_background_color = "#ffffff", #"#042333",
  title_slide_background_image = "../../../files/theme/LSE/spectra_close.jpg",
#    "https://upload.wikimedia.org/wikipedia/commons/1/1a/Workhouse_Nantwich.jpg",
  title_slide_background_size = "cover",
  ) #or contain
```

```{r xaringanextra, include=FALSE, warning=FALSE}
library(xaringanExtra)
#xaringanExtra::use_animate_all("slide_left")
xaringanExtra::use_tile_view()
xaringanExtra::use_panelset()
xaringanExtra::style_panelset_tabs(font_family = "inherit")
```

```{r tidyverse, include=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
theme_set(theme_minimal(base_size = 22))
set.seed(1)
library(broom)
library(modelr)
```

<style type="text/css">
.remark-slide-content {
    font-size: 1.2rem;
    padding: 1em 4em 1em 4em;
}
</style>

## Regression and classification trees

### More interpretable than linear models?

- Sequence of simple questions about individual predictors

- Growing and pruning

### Strategies for improving "weak" models

- Bagging

- Random forests (similar to "dropout" -- future topic)

- Boosting

---

## Decision trees

### Are you eligible for the COVID-19 vaccine?

- If `Age >= 50` then `yes`, otherwise continue
- If `HighRisk == TRUE` then `yes`, otherwise continue
- If `Job == CareWorker` then `yes`, otherwise `no`

This is (arguably) more interpretable than a linear model with multiple predictors

(This is just an example, find the real vaccination criteria [here](https://www.nhs.uk/conditions/coronavirus-covid-19/coronavirus-vaccination/coronavirus-vaccine/))

---

[Penguin data](https://education.rstudio.com/blog/2020/07/palmerpenguins-cran) from Palmer Station Antarctica

![](https://education.rstudio.com/blog/2020/07/palmerpenguins-cran/penguin-expedition.jpg)

---

### Measuring our large adult penguins

```{r}
library(palmerpenguins)
pg <- penguins %>% drop_na()
```

```{r penguinplot, echo = FALSE, fig.width = 10, fig.height = 5.5, fig.align='center'}
pg_bmg_plot <- pg %>% 
  ggplot(aes(x = flipper_length_mm,
             y = bill_length_mm,
             color = body_mass_g)) +
  geom_point() +
#  geom_vline(xintercept = 208) +
  scale_color_viridis_c(end = .8, option = "D") 
#  facet_wrap(vars(sex)) +  theme(axis.text.x = element_blank())
pg_bmg_plot
```

---

### Regression tree to predict penguin massiveness

```{r penguintree, fig.height=5, fig.width=10, fig.align='center'}
library(tree)
fit_tree <- 
  tree(body_mass_g ~ flipper_length_mm + bill_length_mm, control = tree.control(nrow(pg), mindev = 0.007), data = pg) #<<
plot(fit_tree, type = "uniform")
text(fit_tree, pretty = 0, cex = 1.7)
```


---

#### Partial dependence plots with `plotmo`

```{r plotmotree, fig.width = 10, fig.height = 6, fig.align='center'}
library(plotmo)
vars <- c("bill_length_mm", "flipper_length_mm")
plotmo(fit_tree, trace = -1, degree1 = NULL, degree2 = vars)
```



---

### Recursive rectangular splitting on predictors

"Stratification of the feature space"

```
Input: subset of data
  For each predictor variable x_j in subset
    Split left: observations with x_j < cutoff
    Split right: observations with x_j >= cutoff
    Predict constants in each split
    Compute model improvement
    Scan cutoff value to find best split for x_j
Output: predictor and split with best improvement
```

--

Starting from full dataset, compute first split as above

**Recurse**: take the two subsets of data from each side of the split and plug them both back into the same function

Until some **stopping rule** prevents more splitting

---

### Regression tree predictions

```{r penguinctreeplot, echo = FALSE, fig.width = 10, fig.height = 7, fig.align='center'}
pg %>% 
  ggplot(aes(x = flipper_length_mm,
             y = bill_length_mm)) +
  scale_color_viridis_d(end = .8, option = "D")  +
  geom_point(aes(color = as.factor(round(predict(fit_tree)))),
             alpha = .6) +
  geom_vline(xintercept = 206.5, size = 1.5) + 
  geom_vline(xintercept = 193.5, size = 1.5, linetype = "dashed") + 
  geom_vline(xintercept = 214.5, size = 1.5, linetype = "dashed") + 
  geom_segment(x = 214.5,
               y = 48.55,
               xend = 240,
               yend = 48.55, size = 1.5, linetype = "dotdash")  + 
  geom_segment(x = 170,
               y = 38.15,
               xend = 193.5,
               yend = 38.15, size = 1.5, linetype = "dotdash") +
  labs(color = "Predicted\nmass_g")
```


---

### Tree diagram again for comparison


```{r penguintreediagramagain, echo = FALSE, fig.align='center',  fig.height=7, fig.width=14, }
plot(fit_tree, type = "uniform")
text(fit_tree, pretty = 0, cex = 1.7)
```

---

### Categorical predictors

```{r penguinctree, fig.height=4, fig.width=9, fig.align='center'}
fit_tree <- tree(body_mass_g ~ ., data = pg)
plot(fit_tree, type = "uniform")
text(fit_tree, pretty = 0, cex = 1.7)
```

Split using `levels`, e.g. the species `r levels(pg$species)`

---


### Stopping rules

```{r}
fit_tree <- tree(body_mass_g ~ .,
      control = tree.control(nrow(pg), mindev = 0.001), data = pg) #<<
```


```{r penguinbigtree, echo = FALSE, fig.height=6, fig.width=14, fig.align='center'}
plot(fit_tree, type = "uniform")
text(fit_tree, pretty = 3, cex = 1.7)
```

Interpretable?... (see `?tree.control` for options)

---

## Complexity and overfitting

Could keep recursively splitting on predictor space until we have bins containing only 1 unique set of predictor values each

This would be like 1-nearest neighbors

**Lab exercise**: create a plot of training error versus tree size

```{r}
fit_tree <- tree(body_mass_g ~ .,
      control = tree.control(nrow(pg), mindev = 0.000001), data = pg) #<<
summary(fit_tree)$size # number of "leaf" endpoints
```

---

## Growing and pruning

#### Problem: greedy splitting

Each split uses the best possible predictor, similar to forward stepwise. Early stopping may prevent the model from finding useful but weaker predictors later on

**Solution**: don't use early stopping. Grow a large tree

#### Problem: overfitting

Larger trees are more complex, more difficult to interpret, and could be overfit to training data

**Solution**: (cost complexity / weakest link) pruning

---

### How to prune a tree

After growing a large tree, find the "best" sub-tree

#### Problem: too many sub-trees

The number of sub-trees grows combinatorially in the number of splits (depends on depth as well, interesting counting problem)

**Solution**: consider only a one-dimensional path of sub-tree models, the ones that minimize

$$RSS(\text{Sub-tree}) + \alpha |\text{SubTree}|$$

for $\alpha \geq 0$. Now we can choose $\alpha$, and therefore a specific sub-tree, using validation

---

## Classification trees

If the outcome is categorical we need to modify the splitting algorithm

- When making a split, classify all observations in each leaf with the same class (modal category rather than mean numeric prediction)

- Can't measure improvement in fit by reduction in RSS, instead, use reduction of some measure related to classification error

Software generally uses **Gini index** by default. In a leaf:

$$\sum_{k=1}^K \hat p_k(1-\hat p_k) $$

---

## Trees, forests, and other models

- Model using a single tree is very simple. High interpretability, but likely low prediction accuracy

- For proper *machine learning* we'll combine many trees into one model (next topic)

- When should we use these tree methods?
  - High complexity, so usually want $n > p$
  
  - If "true" relationships are linear/smooth, tree methods may fit poorly compared to linear/smooth methods

  - Trees more easily handle categorical predictors and missing values (can treat missingness as a category)

---

### Tree-based fit vs smooth fit

```{r smoothvstree, echo = FALSE, fig.height=10, fig.width=14, fig.align='center', message = FALSE}
library(gapminder)
library(xgboost)
gm2007 <- gapminder %>% filter(year == 1997)
fit_boost <- xgboost(data = as.matrix(gm2007$gdpPercap), label = gm2007$lifeExp, eta = .05, nrounds = 80, max_depth = 2, lambda = 1, verbose = 0)
gm2007 %>%
  ggplot(aes(gdpPercap, lifeExp)) +
  geom_point() +
  geom_smooth() +
  geom_line(aes(y =
                  predict(fit_boost,
                          newdata = as.matrix(gm2007$gdpPercap))),
            color = "green", size = 1)
```

