---
title: "Machine learning"
subtitle: "Additive non-linearity"
author: "Joshua Loftus"
#institute: "LSE"
#date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "xaringan-themer.css"]
    seal: true    
    lib_dir: libs
    nature:
      titleSlideClass: ["bottom", "left"]
      countdown: 59000
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
---

class: inverse

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE)
options(knitr.table.format = "html")
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#2d708e",
  secondary_color = "#230433",
  link_color = "#55c667",
  text_bold_color = '#f68f46',
  title_slide_background_color = "#ffffff", #"#042333",
  title_slide_background_image = "../../../files/theme/LSE/spectra_close.jpg",
#    "https://upload.wikimedia.org/wikipedia/commons/1/1a/Workhouse_Nantwich.jpg",
  title_slide_background_size = "cover",
  ) #or contain
```

```{r xaringanextra, include=FALSE, warning=FALSE}
library(xaringanExtra)
xaringanExtra::use_animate_all("slide_left")
```

```{r tidyverse, include=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
theme_set(theme_minimal(base_size = 22))
set.seed(1)
library(broom)
library(modelr)
```

<style type="text/css">
.remark-slide-content {
    font-size: 1.2rem;
    padding: 1em 4em 1em 4em;
}
</style>

## Additive

separate non-linear terms are combined by addition

## univariate

each non-linear term uses only one predictor

## non-linearity

can be fit using various methods we've already learned

### GAM: **G**eneralized **A**dditive **M**odel

---

## Additive modeling assumption

- **Linearity** assumption: each predictor has a *coefficient*

$$g(\mathbb E[\mathbf y | \mathbf X]) = \beta_0 + \beta_1 \mathbf x_1 + \beta_2 \mathbf x_2 + \cdots + \beta_p \mathbf x_p$$
- **Additivity** assumption: each predictor has a *function*

$$g(\mathbb E[\mathbf y | \mathbf X]) = \beta_0 + f_1( \mathbf x_1) + f_2( \mathbf x_2) + \cdots + f_p(\mathbf x_p)$$

Includes linear models as special case if $f_j(\mathbf x_j) = \beta_j \mathbf x_j$

Assumptions / modeling choices:

- Assume $f_j$ is in some function space / fit with some method
- e.g. global polynomial, `loess`, local/kernel regression, smoothing splines, etc--pick your favorite!
- Can use same/different methods for each predictor


---

### Non-linear regression

Other times it's less clear, based on noise level and sample size


```{r nonlinear-reg-add}
f1 <- function(x) -1 + 2*x - x^2
f2 <- function(x) sin(pi*x)
f3 <- function(x) exp(-5*(x-1/2)^2)

set.seed(1)
n <- 400
df <- data.frame(
  x1 = 2*(runif(n)-1/2),
  x2 = sample(1:100 / 50,  n, replace = TRUE),
  x3 = runif(n)
) %>%
  mutate(
    y = f1(x1) + f2(x2) + f3(x3) + rnorm(n) #<<
)
```

---

### Univariate plots

```{r}
uni_plot <- function(j) {
  xj <- paste0("x", j)
  fj <- paste0("f", j)
  ggplot(df, aes(get(xj), y)) +
    geom_point(alpha = .5) +
    geom_smooth() + xlab(xj) +
    geom_function(fun = get(fj),
                  size = 1,
                  color = "green") +
    theme(axis.text.x=element_blank(),
          axis.ticks.x=element_blank())
}
p1 <- uni_plot(1)
p2 <- uni_plot(2)
p3 <- uni_plot(3)
```

Side by side plots by adding with the `patchwork` library

---

```{r fig.width = 10, fig.height = 6, fig.align='center'}
library(patchwork)
p1 + p2 + p3
```

---

## Bias? Why? `r emo::ji("astonished")`

The true model is additive

We plot each variable separately but the `loess` curves are biased...


To fit $\hat f_1$ we would *ideally* do `loess` on

$$
y - f_2(\mathbf x_2) - f_3(\mathbf x_3)
$$

But we don't know $f_2$ and $f_3$, we are trying to estimate them too!

---

### **Backfitting** algorithm

1. Start with some initial estimates $\hat f_j$, e.g. from `y ~ x_j`

2. Iterate over $j$, updating $\hat f_j$ by fitting `r_j ~ x_j` where the partial residual $\mathbf r_j$
$$\mathbf r_j = \mathbf y - \hat \beta_0 - \sum_{k \neq j} \hat f_k(\mathbf x_k)$$
is computed using the current fits for all the other predictors

3. Repeat until "convergence" (some stopping rule)

---

### Can additivity/GAMs be *importantly wrong*?

Interpretation: think carefully about **calculus** and **causality**. To simplify let's consider the identity link function (rather than e.g. logistic regression, those cases are more complicated)

#### Calculus

Does the CEF really decompose into additive terms?  Is this approximation good:

$$
\frac{\partial}{\partial x_j} \mathbb E[Y | \mathbf X] \approx g(x_j)
$$
Or does the relationship between the average of $Y$ and $x_j$ vary depending on the value of another predictor $x_k$?

---

### Can additivity/GAMs be *importantly wrong*?

Interpretation: think carefully about **calculus** and **causality**. To simplify let's consider the identity link function (rather than e.g. logistic regression, those cases are more complicated)

#### Causality

First, remember that causality is separate from prediction

But also, it may be a reason for doubting additivity

For example, if $X_k$ is a cause of $X_j$, or if they have a common cause, then we may want to include an interaction term for them



---


```{r}
library(ggplot2movies)
df <- movies %>% 
  filter(length <= 200, length > 10,
         year > 1918, votes >= 5) #, Short != 1)
```


I asked on [Twitter](https://twitter.com/tslumley/status/1361789344118284288) what was missing from the plot of movie length vs movie rating and Thomas Lumley suggested confounding by **year**


---

### Additive combination of non-linear predictors

```{r}
library(gam)
fit_gam_loess <- 
  gam(rating ~ lo(length) + lo(year), data = df)
```

`lo` is for `loess`, but can use different methods too

```{r}
tidy(fit_gam_loess)
```
No coefficients, so how do we interpret?

---

### Replace each linear coefficient with 2d plot

```{r fig.width = 9, fig.height = 5.5, fig.align='center'}
par(mfrow = c(1,2))
plot(fit_gam_loess)
```


---

### Interpretation: holding other variables constant

```{r}
df_hat <- df %>% 
  mutate(.fitted = predict(fit_gam_loess))

df_fixed_year <- df_hat %>%
  filter(year %in% c(1950, 1960, 1970, 1980, 1990, 2000))

df_fixed_length <- df_hat %>%
  filter(length %in% c(80, 100, 120))
```

Let's look at a few specific years and plot the **fitted relationship** with length for each of those subsets of the data

Do the same for a few specific lengths and **fitted relationship** with year

---

### "Coefficient" of length, holding year constant

```{r fig.width = 9, fig.height = 5.5, fig.align='center'}
df_fixed_year %>%
  ggplot(aes(length, rating)) + geom_point(alpha = .1) +
  geom_line(aes(y = .fitted)) + facet_wrap(~ year)
```

---

### "Coefficient" of year, holding length constant

```{r fig.width = 9, fig.height = 5.5, fig.align='center'}
df_fixed_length %>%
  ggplot(aes(year, rating)) + geom_point(alpha = .1) +
  geom_line(aes(y = .fitted)) + facet_grid(~ length) + theme(axis.text.x=element_text(angle = 45)) + scale_x_continuous(breaks = c(1930, 1960, 1990))
```


---

### One univariate non-linear relationship

```{r echo = F, fig.width = 10, fig.height = 7.5, fig.align='center'}
df %>%
  ggplot(aes(length, rating)) +
  geom_point(alpha = .1) + geom_smooth()
```


---

### Another univariate non-linear relationship

```{r echo = F, fig.width = 10, fig.height = 7.5, fig.align='center'}
df %>%
  ggplot(aes(year, rating)) +
  geom_point(alpha = .1) + geom_smooth()
```

---

### Interactions in the movies data

Does the relationship between length and rating change depending on the year? Let's check a few years

```{r echo = F, fig.width = 10, fig.height = 6, fig.align='center'}
df_fixed_year %>%
  ggplot(aes(length, rating)) + geom_point(alpha = .1) +
  geom_smooth() + facet_wrap(~ year)
```

---

### Misspecification: failure of additivity

Difficult to tell because of small $n$ outside the range of length between 1 and 2 hours

But I think it's possible the *relationship* is changing over time, i.e. there is an interaction


$$
\frac{\partial}{\partial \text{length}} \mathbb E[\text{rating} | \text{length}, \text{year}] \approx g(\text{length}, \text{year})
$$

Since the right hand side does not depend on length *only*, the additive model might be a poor fit

Less accurate predictions

(Possibly importantly) wrong interpretations

---

# "Linear modeling **assumption**"

Why are we so often *assuming* linearity? (of the right hand side)

$$
g(\mathbb E[\mathbf y]) = \beta_0 + \beta^T \mathbf x
$$

- Easier to interpret, sure...
- But also easier to estimate

Sometimes non-linearity is clear from the data or domain info

Other times it's less clear, and makes it harder to learn a CEF


---

## Fundamental limits in non-linearity

Applies to many ML approaches

- GAMs (Generalized Additive Models)
- Nearest neighbors
- Kernels
- Trees
- Networks (deep learning)

(Can use any for both **regression** and **classification**)

---

### Non-linear regression

.pull-left[
```{r nonlinear-reg-simple, echo = FALSE}
f <- function(x) -1 + 2*x - x^2
g <- function(x) sin(100*x)
gen_f <- function(x) {
  data.frame(x = x) %>%
  mutate(y = f(x) + rnorm(length(x)))
}
gen_fg <- function(D) {
  D %>%
    mutate(y = y + g(x))
}
set.seed(1)
n <- 400
x <- 2*runif(n) + .5
data_f <- gen_f(x)
plot_f <- ggplot(data_f, aes(x,y)) +
  geom_point(alpha = .5) +
  geom_smooth(se = FALSE, method = "lm", size = 2, linetype = "dashed") +
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank()) 
plot_f  +
  geom_line(aes(x,f(x)), color = "#00aa00", size = 1,
            data = data_grid(data_f, x = seq_range(x, 1000)))
```
]
.pull-right[
```{r  nonlinear-reg-sin, echo = FALSE}
data_fg <- gen_fg(data_f)
plot_fg <- ggplot(data_fg, aes(x,y)) +
  geom_point(alpha = .5) +
  geom_smooth(se = FALSE, method = "lm", size = 2, linetype = "dashed") +
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank())
plot_fg  +
  geom_line(aes(x,f(x)), color = "#00aa00", size = 1,
            data = data_grid(data_fg, x = seq_range(x, 1000)))
```

]

One CEF is $f(x) = -1 + 2x - x^2$, the other is $f(x) + g(x)$

---

### Fitting the "true" models

.pull-left[
```{r}
fit <- function(D) {
  list(
  lm(y ~ x, D),
  lm(y ~ f(x), D),
  lm(y ~ f(x) + g(x), D)) 
}
models_data_f <- 
  fit(data_f)
models_data_fg <- 
  fit(data_fg)
```
Lists of fitted models on each dataset
- Linear (underfit?)
- $f(x)$
- $f(x) + g(x)$
]
.pull-right[
```{r}
models_data_f
```
]

---

```{r highlight.output = c(6)}
map_dfr(models_data_f, glance) # true CEF = f
```

```{r highlight.output = c(6)}
map_dfr(models_data_fg, glance) # CEF = f + g
```

Both look like high noise level, but 1 has ~double $R^2$? `r emo::ji("face_with_raised_eyebrow")` 

---

### Revealing $f(x) + g(x)$ `r emo::ji("zany_face")` 

.pull-left[
```{r echo = FALSE}
plot_f +
  geom_line(aes(x,f(x)), color = "#00aa00", size = 1,
            data = data_grid(data_f, x = seq_range(x, 1000)))
```
]
.pull-right[
```{r echo = FALSE}
plot_fg +
  geom_line(aes(x,f(x) + g(x)), color = "#00aa00", size = 1,
            data = data_grid(data_fg, x = seq_range(x, 1000)))
```
]

Datasets *look* very similar, but $f+g$ fits one and not the other


---

## If not linear, then what?

Choose a **space of functions** to optimize over

- Linear functions in $p$ variables $\leftrightarrow$ vector space $\mathbb R^p$

- Polynomials up to a fixed, maximum degree: also finite dimensional vector space

- Many (non-linear) function spaces are **infinite dimensional** vector spaces

  - $\{ f_k(x) = \sin(k \pi x) : k \in \mathbb Z \}$ (Fourier basis)
  
  - Spaces of integrable functions, or differentiable

- Underlying math: linear algebra $\to$ functional analysis

---

### Intuitions about function spaces

- Optimize over a larger space $\to$ fit more complex models

- Bias-variance trade-off: *both* choice of right/good space of functions *and* amount of complexity in that space

  - e.g. periodic (like last example), right wavelengths
  
  - e.g. smooth, right amount of wiggliness
  
  - e.g. "Shape constraints" like monotonic, unimodal, (log-)concave (*Application*: epidemic trajectory)

Science/modeling/inference approach: domain knowledge, first principles

ML approach: whichever function space has current SOTA software (with easy to use default settings `r emo::ji("grinning_squinting_face")`)

---

### Optimizing over a large function space

```{r}
overfit <- function(D, k_range = 0:200) {
  fit_sin_k <- function(k) {
    fit_k <- lm(y ~ x + sin(k*x), data = D)
    glance(fit_k)$r.squared
  }
  r_squareds <- map_dbl(k_range, fit_sin_k)
  best_k <- k_range[which.max(r_squareds)]
  best_k
}
khat_f <- overfit(data_f)
khat_fg <- overfit(data_fg)
c(khat_f, khat_fg)
```

$$
\hat f(x) = \beta_0 + \beta_1 x + \beta_2 \sin(\hat k x)
$$

Apparently $\hat k = 1$ or $\hat k = 100$, respectively

---

### Plotting the "best" models

```{r echo = FALSE}
fhat_f <- function(x) sin(khat_f*x)
fhat_fg <- function(x) sin(khat_fg*x)
best_model_f <- lm(y ~ x + fhat_f(x), data = data_f)
best_model_fg <- lm(y ~ x + fhat_fg(x), data = data_fg)
```

.pull-left[
```{r echo = FALSE}
# signal_plot <- ggplot(data_f, aes(x,y)) +
#   geom_point(alpha = .5) +
#   geom_smooth(se = FALSE, method = "lm", linetype = "dashed", color = "#00aa00", size = 2) +
#   theme(
#   axis.text.x = element_blank(),
#   axis.text.y = element_blank())
# signal_
plot_f +
  #ggtitle("The 'best' (overfit) model") +
  geom_line(aes(x, .fitted), color = "blue", size = 1,
            data = augment(best_model_f,
                           newdata = data_grid(data_f,
                           x = seq_range(x, 1000))))
```
]
.pull-right[
```{r echo = FALSE}
plot_fg +
  #ggtitle("The 'best' (overfit) model") +
  geom_line(aes(x, .fitted), color = "blue", size = 1,
            data = augment(best_model_fg,
                           newdata = data_grid(data_fg,
                           x = seq_range(x, 1000))))
```
]

Can we believe this?

---

### So which is it?

When we aren't doing simulations we just have the data

.pull-left[
```{r echo = F}
plot_f + ggtitle("Test data could prevent overfitting")
```
]
.pull-right[
```{r echo = F}
plot_fg + ggtitle("Doomed to underfit?")
```
]

We don't know signal/noise level, function space, complexity...

---

### The "big data" advantange

With larger samples we could tell these two cases apart

.pull-left[
```{r echo = F}
set.seed(1)
n <- 10000
x <- 2*runif(n) + .5
bigdata_f <- gen_f(x)
bigdata_fg <- gen_fg(bigdata_f)
noise_plot <- ggplot(bigdata_f, aes(x,y)) +
  geom_point(alpha = .7) +
  geom_smooth(se = FALSE, method = "lm", size = 2, linetype = "dashed") +
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank())
noise_plot #+
    #geom_line(aes(x, .fitted), color = "blue", size = 2, data = augment(best_model_f, newdata = data_grid(bigdata_f, x = seq_range(x, 1000))))
```
]
.pull-right[
```{r echo = F}
signal_plot <- ggplot(bigdata_fg, aes(x,y)) +
  geom_point(alpha = .7) +
  geom_smooth(se = FALSE, method = "lm", linetype = "dashed", size = 2) +
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank())
signal_plot #+
  #geom_line(aes(x, .fitted), color = "blue", size = 2, data = augment(best_model_fg, newdata = data_grid(bigdata_fg, x = seq_range(x, 1000))))
```
]

Use more data for validation / in-distribution generalization

---

## Non-linearity and overfitting

Much of machine learning and "AI" is about having large enough datasets to search large spaces of functions and fit complex models without **variability problems** from overfitting

i.e. good in-distribution generalization (new data, same DGP)

#### Intuition: more complex models are more sensitive to small changes in the data, or more "brittle"

#### Statistical wisdom: another reason to prefer simpler models may be better out-of-distribution generalization

i.e. avoiding **bias problems** from overfitting

---

### Out-of-distribution generalization

What if we test on data outside the original range/distribution?

.pull-left[
```{r echo = F}
set.seed(1)
n <- 1000
x <- 5*runif(n) + .5
bigdata_f <- gen_f(x)
bigdata_fg <- gen_fg(bigdata_f)
noise_plot <- ggplot(bigdata_f, aes(x,y)) +
  geom_point(alpha = .7) +
  geom_smooth(se = FALSE, method = "lm", size = 2, linetype = "dashed") +
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank())
noise_plot +
    geom_line(aes(x, .fitted), color = "blue", size = 2,
            data = augment(best_model_f,
                           newdata = data_grid(bigdata_f,
                           x = seq_range(x, 1000))))
```
]
.pull-right[
```{r echo = F}
signal_plot <- ggplot(bigdata_fg, aes(x,y)) +
  geom_point(alpha = .7) +
  geom_smooth(se = FALSE, method = "lm", linetype = "dashed", size = 2) +
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank())
signal_plot +
  geom_line(aes(x, .fitted), color = "blue", size = 2,
            data = augment(best_model_fg,
                           newdata = data_grid(bigdata_fg,
                           x = seq_range(x, 1000))))
```
]

Simpler/"underfit" models (dashed lines) *might* do better

---

### Choosing function spaces and methods

Since this is a course in ML, we won't assume these choices can be informed by domain knowledge

A few examples based on high level **properties of the data** and **goals of the analysis** -- not an exhaustive list or flowchart

(Assuming data shape is rectangular and i.i.d., otherwise we need specialized models for other data/dependence types)

|    Goals   | $n > p$ (tall) | $n \approx p$ or $p > n$ (wide)    |
| :---        |    :----:   |          :---: |
| Prediction only      | Network methods       | Ridge   |
| + Interpretation   | See below        | Lasso      |

Additivity $\to$ GAMs. Interactions $\to$ tree methods


