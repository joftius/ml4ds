<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Machine learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Joshua Loftus" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/animate.css-xaringan/animate.slide_left.css" rel="stylesheet" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="../../../theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: top, left, title-slide

# Machine learning
## Optimization
### Joshua Loftus

---


class: inverse









&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 1.2rem;
    padding: 1em 4em 1em 4em;
}
&lt;/style&gt;

## Recollections

Since the beginning of the term we've seen

- Fitting different kinds of models
  - Least squares (global/linear and local/flexible)
  - Newton-Raphson for logistic regression / GLM / MLE
  - Defining SVMs via optimization problems
  
--
  
- Making models more complex
  - Adding variables
  - More flexible functions
  
--

This lecture: **optimization** as a unifying framework for ML

---

## ML tasks as optimization problems

For a given **loss function** `\(L(x,y,g)\)` and **probability model** `\((X,Y) \sim P\)`, we want to minimize the **risk**

$$
\text{minimize } R(g) = \mathbb E_P[L(X,Y,g)]
$$
--

**Sampling assumption**: we have an i.i.d. sample from `\(P\)`

Then we focus on **empirical risk minimization** (ERM)

$$
\text{minimize } \frac{1}{n} \sum_{i=1}^n L(x_i, y_i, g)
$$

--

We also choose a **function class** (or parameter set), i.e. type of `\(g\)`, determining the domain of the optimization


---

### Example: GLMs

- Probability model: the family of the GLM (e.g. binomial)
- Empirical loss function: negative log-likelihood (MLE)
- Function class: set of (parametric) functions of the form

$$
f_\beta(\mathbf x) = g^{-1}(\mathbf x^T \beta)
$$
for some `\((p+1)\)`-dimensional vector of parameters, fixed link function `\(g\)`

- Make it more complex:
  - Add more predictors (new ones, or non-linear transformations, interactions, etc)
  - Localize it: for some flexible, non-linear `\(h\)` (e.g. `loess`) 
  
  $$
  f(\mathbf x) = g^{-1}(h(\mathbf x))
  $$

---

### Parametric vs "non"-parametric

For G**L**M, Newton-Raphson gives us `\(\hat \beta\)`, i.e. `\(\hat f\)`

For `loess`, we choose (or use the default) `span` and then do local logistic regression to get `\(\hat f\)`

(using a multi-dimensional analogue of `loess` if necessary)

--

In both cases, we have computed the empirical risk minimizer `\(\hat f\)` over some function class

--

**Key optimization choice**: which function class?

- Multivariate linear case: which predictors to include?
- `loess` case: what `span` value?

---

### Example: SVM

- Probability model: ...
- Empirical loss function: **hinge** loss, `\([\ ]_+\)` means positive part

`$$\frac{1}{n} \sum_{i=1}^n [1 - y_i(\mathbf x_i^T \beta - \beta_0)]_+$$`

Next slide: ISLR Figure 9.2 plot of `\(L(y_i \hat y_i) = [1-y_i\hat y_i]_+\)`

- Function class: linear classifiers `\(f(\mathbf x) = \text{sign}(\mathbf x^T \beta + \beta_0)\)`

- Make it more complex

  - Add more predictors
  - Reduce constraint-violation budget (non-sep. case)
  - Make it flexibly non-linear (future lecture)

---

### Example: SVM

&lt;img src="islr9.12.png" width="80%" /&gt;

---

### Comparison: logistic vs SVM

- Similar classification performance possible with both
  - Compare "apples to apples," e.g. both non-linear
- Logistic advantage: inference
- SVM advantage: computation

--

**Key optimization choice**: which function class?

- Multivariate linear case
  - Which predictors to include?
  - What constraint budget value?
- Non-linear case: choosing the analogue of the `span` value

---

### ML design choices

Pattern repeats as we introduce more ML methods (kernel methods, tree-based methods, neural networks, etc)

- Loss function (RSS, MLE, hinge, etc)

--

- Class of regression/classification functions (linear, parametric non-linear, some specific kind of non-parametric non-linear like `loess`, etc)

--

- Algorithm for fitting an optimal function *within* that class (choosing which predictors to include, estimating coefficients, choosing `span` or SVM constraint budget, etc)

--

**Statistics** ...often forgotten in ML! Key assumptions: **i.i.d.** data **sampled from desired probability distribution** (bias - "dataset/distribution shift")

---
class: inverse, center, middle


## Optimization strategies

### Choosing predictor variables

---

### Best subset selection

![](https://lh3.googleusercontent.com/proxy/GvHYARxXlkjglIBjtStHONXCcuMCDvrxRmVJs4wVtrPRjbwC_5yt3GZyQH3vARN-mCZ-0s9Tnw1Qzsx8mbUEL5bH5FinTeBFTd6CfIrXKnTsTX9CKreGTMXLaPQM189xct_y0QyIgHTLmXrwFPlRtI8ojtErPK8e3R4)

- Try all `\(2^p\)` subsets of predictor variables

- Keep the best one (based on RSS or deviance or something)

--

- Problem: complexity exponential in `\(p\)`, over `\(10^9\)` models if `\(p = 30\)`

---

### Forward stepwise/stagewise selection

"Greedy" approximation to best subset

1. Start with no predictors
2. At each step, find the one predictor (or a few, in stagewise) giving the best improvement (reduction in the loss function) over the current model
3. Add the best predictor(s) and iterate

--

- Greedy: not guaranteed to find the best model

- Computation: only `\(\binom{p}{2}\)` models, e.g. 435 if `\(p = 30\)`

--

- **Problem**: when to stop adding more variables? After how many steps? (We'll come back to this)

---

### Modeling assumption: sparsity

We might be willing to assume that a "true" (good enough) model contains only a few predictors

--

We call this **sparsity**, and may even refer to the number of variables as "the sparsity" of the model, or look for "the best 5-sparse model"

[Motivation](https://en.wikipedia.org/wiki/Occam%27s_razor): Occam's razor / law of parsimony -- simpler models/theories are philosophically/scientifically preferable

--

#### Sparse best subsets

Now only `\(\sum_{k=1}^s \binom{p}{k}\)` models to try, if sparsity assumed `\(\leq s\)`

e.g. about 174000 if `\(p = 30\)` and `\(s = 5\)`

---

### Coming soon: lasso

Another method to choose predictor variables

Based on sparsity assumption

Can think of it as a *less greedy* version of forward stepwise

---
class: inverse, center, middle


## Optimization strategies

### Choosing tuning parameters

![](https://upload.wikimedia.org/wikipedia/en/thumb/0/06/Spinal_Tap_-_Up_to_Eleven.jpg/330px-Spinal_Tap_-_Up_to_Eleven.jpg)

Degree of flexibility for non-linear methods, constraint budget for SVM, etc

---

## Discretize and fit sequentially

- Start with a grid of values for the tuning parameter
- Fit the model for each value in this grid
- Pick the best fit (visually, or based on loss function value, or...)

--

e.g. For the `span` or fraction `\(s\)` in local regression, try `\(s \in \{ 0.1, 0.25, 0.5, 0.75, 0.9 \}\)` and visualize the result

--

e.g. For the budget ("soft margin") `\(C\)` in SVM, try `\(C \in \{ 2^{k} : k = -4, -2, 0, \ldots, 10 \}\)` 

--

- **Problem**: When to stop increasing the complexity? (i.e. decreasing `\(s\)` or `\(C\)`)


---

### Modeling assumption: smoothness

Version of simplicity/parsimony for flexible function classes

--

Linear functions are the smoothest

Smooth function classes: set of functions with some type of bound on second derivatives, for example

--

**Cool math fact**: can be related to sparsity by considering (rate of decay of) coefficients of function's Fourier transform (smoother functions have sparser representations when written in a basis of sine functions, for example)

---
class: inverse, center, middle

## Optimization strategies

### "Scaling up" to "big data"

---

### Calculus with the loss function, revisited

Downside of Newton-Raphson: requires second derivatives, including *inverting the `\(p \times p\)` Hessian matrix* when optimizing over `\(p &gt; 1\)` parameters

If `\(p\)` is large, **second-order** optimization methods like Newton's are very costly

--

First order methods only require computing the `\(p \times 1\)` gradient vector

Recall that the gradient is a vector in the *direction of steepest increase* in the parameter space

---

### Gradient (steepest) descent

i.e. skiing as fast as possible. Notation, let

`$$L(\beta) = L(\mathbf X, \mathbf y, g_\beta)$$`

1. Start at an initial point `\(\beta^{(0)}\)`

2. For step `\(n = 1, \ldots\)`
  - Compute `\(\mathbf d_n = \nabla L(\beta^{(n-1)})\)`
  
  - Update `\(\beta^{(n)} = \beta^{(n-1)} - \gamma_n \mathbf d_n\)`
  
3. Until some convergence criteria is satisfied

--

Where the **step size** `\(\gamma_n\)` is made small enough to not "overshoot" and increase the loss, i.e. the loss only decreases

---

### Coordinate descent

Update only one *coordinate* of `\(\beta\)` in each step

Cycle through coordinates until some convergence criteria is satisfied

--

Can combine with any strategy for univariate optimization -- e.g. one-dimensional Newton's method -- treating other parameters as constants

---
class: inverse, center, middle

## Optimization strategies

### Scale up *more*! Bigger data! 

![](https://upload.wikimedia.org/wikipedia/en/9/9f/Over_9000%21.png)
---

### Stochastic/random descent

- Instead of cycling through all coordinates in coordinate descent, just pick one randomly

- Instead of computing the gradient of the loss function on the entire dataset, compute it on a random sample

--

By identical distribution assumption, for any `\(i'\)`, by linearity ðŸŒ  of `\(\nabla\)` and `\(\mathbb E\)` and `\(\sum\)`,

`$$\mathbb E [ \nabla L(\mathbf x_{i'}, \mathbf y_{i'}, g_\beta) ] = \mathbb E \left[ \frac{1}{n} \sum_{i=1}^n \nabla L(\mathbf x_i, \mathbf y_i, g_\beta) \right]$$`
--

Compute update using one randomly sampled observation

or a randomly sampled subset ("mini-batch SGD")

---
class: inverse, center, middle

## Optimization strategies

### A few special topics in conclusion

---

### Constrained optimization

Remember, some of our optimization problems have constraints on the parameters, e.g. SVM

**Problem**: What if the steps in these descent methods take us outside the parameter constraint region?

--

**Solution strategy**: Choose step sizes small enough to stay inside the constraint region

**Solution strategy**: *Project* from the updated point that is outside the constraint region to the *nearest* point inside the constraint region

---

### Non-smooth optimization

**Problem**: What if the loss function is not (everywhere) differentiable?

And suppose it is *still [convex](https://en.wikipedia.org/wiki/Convex_function)*, e.g. hinge loss, absolute value, etc

--

**Solution strategies**:  In this case there is not a well-defined gradient but there is still something called a *[subgradient](https://en.wikipedia.org/wiki/Subgradient_method)* which acts like a set of values that are all potential gradients--they all define tangent lines (surfaces) that *stay below the function*

Now if we're at a non-differentiable point we just need to compute any subgradient value and take a step in that direction

([Advanced topic](https://en.wikipedia.org/wiki/Proximal_gradient_method), this slide non-examinable)

---

### Early stopping

#### Optimization time = complexity

- For many optimization algorithms (including those on previous slides) the fitted model becomes more complex the longer the optimization algorithm runs

--

  - e.g. the more steps of (stochastic) gradient descent used in combination with a flexible function class
  
  - e.g. the more steps of forward stepwise (adding more predictor variables)

--
  
**Idea**: control model complexity by stopping the algorithm before convergence 

This is [early stopping](https://en.wikipedia.org/wiki/Early_stopping) -- we'll come back to it later

---
class: inverse

## Optimization theory

- If the loss function is convex many of these methods have *guaranteed convergence* to the *global minimizer*

--

- If the loss function is non-convex, we lose mathematical guarantees

  - Possible convergence to local minimizer
  
  - Local minimizers may be much worse than the best possible model...
  
  - Or they might not be!

--

Deep learning: to hell with convexity ðŸ¤  "it just works"

---
class: inverse, center, middle

### Conclusion: optimization in ML is a big topic

#### Strategies for specific problems

e.g. stepwise inclusion of variables, constraints, etc

#### Strategies for general loss/function classes

e.g. gradient methods, coordinate methods

#### Stopping at the right amount of complexity

*Maybe the most important part! Next lecture*
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"highlightSpans": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
(function(time) {
  var d2 = function(number) {
    return ('0' + number).slice(-2); // left-pad 0 to minutes/seconds
  },

  time_format = function(total) {
    var secs = Math.abs(total) / 1000;
    var h = Math.floor(secs / 3600);
    var m = Math.floor(secs % 3600 / 60);
    var s = Math.round(secs % 60);
    var res = d2(m) + ':' + d2(s);
    if (h > 0) res = h + ':' + res;
    return res;  // [hh:]mm:ss
  },

  slide_number_div = function(i) {
    return document.getElementsByClassName('remark-slide-number').item(i);
  },

  current_page_number = function(i) {
    return slide_number_div(i).firstChild.textContent;  // text "i / N"
  };

  var timer = document.createElement('span'); timer.id = 'slide-time-left';
  var time_left = time, k = slideshow.getCurrentSlideIndex(),
      last_page_number = current_page_number(k);

  setInterval(function() {
    time_left = time_left - 1000;
    timer.innerHTML = ' ' + time_format(time_left);
    if (time_left < 0) timer.style.color = 'red';
  }, 1000);

  slide_number_div(k).appendChild(timer);

  slideshow.on('showSlide', function(slide) {
    var i = slide.getSlideIndex(), n = current_page_number(i);
    // reset timer when a new slide is shown and the page number is changed
    if (last_page_number !== n) {
      time_left = time; last_page_number = n;
      timer.innerHTML = ' ' + time_format(time); timer.style.color = null;
    }
    slide_number_div(i).appendChild(timer);
  });
})(59000);
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
