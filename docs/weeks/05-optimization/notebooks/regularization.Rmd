---
title: "Regularization"
author: "LSE ST310"
date: "18/02/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#library(tidyverse)
```

## Early stopping

Copy/paste your gradient descent (or stochastic version) code for ordinary linear regression here and use it to try regularization via early stopping

Consider generating data with p larger than n, so the least squares estimator is undefined

```{r}

```


## Ridge regression with glmnet

Use the `glmnet` package to fit a ridge regression model on the same data as in the previous part. Hint: read about the `alpha` input to the `glmnet` function in the documentation.

```{r message = FALSE}
# install.packages("glmnet") # if necessary
library(glmnet)
```

```{r}

```

What does plotting the model object show?

```{r}

```

Try the functions in the `broom` package on the model object

```{r}

```

## Cross-validation

Write a function to randomly split the data into subsets and implement k-fold cross-validation. You can use the `glmnet` function, `broom` functions, but do not use `cv.glmnet` or any other packages that implement cross-validation. The point is to manually implement iterating over the different subsets, computing RSS, and averaging the results

```{r}

```

